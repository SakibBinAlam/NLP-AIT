{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PThyRORoyZO-"
      },
      "source": [
        "# Code Autocompletion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DaPIGOqhyZPD"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0eQ9fMVyZPF",
        "outputId": "e75790d3-5f8b-4de1-94b3-3201d0a1586a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "#make our work comparable if restarted the kernel\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLRrW9tDyZPG"
      },
      "source": [
        "# 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQTEFuwxyx79",
        "outputId": "db3f107e-d357-4df6-d42c-d7ee3c3c9acd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.10.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpwoqVNvyZPG",
        "outputId": "5b1ff630-522c-4865-b0f8-2ca9fa8eb863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['repo_name', 'path', 'license', 'content'],\n",
            "    num_rows: 47452\n",
            "}) Dataset({\n",
            "    features: ['repo_name', 'path', 'license', 'content'],\n",
            "    num_rows: 11864\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "import datasets \n",
        "train = datasets.load_dataset(\"codeparrot/github-jupyter-code-to-text\", split=\"train\")\n",
        "test = datasets.load_dataset(\"codeparrot/github-jupyter-code-to-text\", split=\"test\")\n",
        "print(train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "KX8Z-_0uyZPH",
        "outputId": "efa4d98d-b85b-4e4b-b5ec-27b6c5f489fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import numpy as np\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\n\\n\"\"\"\\nExplanation: Simple MNIST convnet\\nAuthor: fchollet<br>\\nDate created: 2015/06/19<br>\\nLast modified: 2020/04/21<br>\\nDescription: A simple convnet that achieves ~99% test accuracy on MNIST.\\nSetup\\nEnd of explanation\\n\"\"\"\\n\\n\\n# Model / data parameters\\nnum_classes = 10\\ninput_shape = (28, 28, 1)\\n\\n# the data, split between train and test sets\\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\\n\\n# Scale images to the [0, 1] range\\nx_train = x_train.astype(\"float32\") / 255\\nx_test = x_test.astype(\"float32\") / 255\\n# Make sure images have shape (28, 28, 1)\\nx_train = np.expand_dims(x_train, -1)\\nx_test = np.expand_dims(x_test, -1)\\nprint(\"x_train shape:\", x_train.shape)\\nprint(x_train.shape[0], \"train samples\")\\nprint(x_test.shape[0], \"test samples\")\\n\\n\\n# convert class vectors to binary class matrices\\ny_train = keras.utils.to_categorical(y_train, num_classes)\\ny_test = keras.utils.to_categorical(y_test, num_classes)\\n\\n\"\"\"\\nExplanation: Prepare the data\\nEnd of explanation\\n\"\"\"\\n\\n\\nmodel = keras.Sequential(\\n    [\\n        keras.Input(shape=input_shape),\\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\\n        layers.MaxPooling2D(pool_size=(2, 2)),\\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\\n        layers.MaxPooling2D(pool_size=(2, 2)),\\n        layers.Flatten(),\\n        layers.Dropout(0.5),\\n        layers.Dense(num_classes, activation=\"softmax\"),\\n    ]\\n)\\n\\nmodel.summary()\\n\\n\"\"\"\\nExplanation: Build the model\\nEnd of explanation\\n\"\"\"\\n\\n\\nbatch_size = 128\\nepochs = 15\\n\\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\\n\\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\\n\\n\"\"\"\\nExplanation: Train the model\\nEnd of explanation\\n\"\"\"\\n\\n\\nscore = model.evaluate(x_test, y_test, verbose=0)\\nprint(\"Test loss:\", score[0])\\nprint(\"Test accuracy:\", score[1])\\n\\n\"\"\"\\nExplanation: Evaluate the trained model\\nEnd of explanation\\n\"\"\"\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Let's take a peek at the dataset\n",
        "train[\"content\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VowP3vuyZPI"
      },
      "source": [
        "# 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FrNyF1-GyZPI"
      },
      "outputs": [],
      "source": [
        "# Making the data in desired format ex: \"import numpy as np\", \"from tensorflow import keras\"...\n",
        "\n",
        "train_split = [split for text in train['content'] for split in text.split('\\n') if split != \"\"]\n",
        "test_split = [split for text in test['content'] for split in text.split('\\n') if split != \"\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFvY1ruhyZPJ",
        "outputId": "2f0afd0d-e9cc-4319-e267-7f87d698a93d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11367363, 2875424)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(train_split), len(test_split)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmdGhLaIzJp6",
        "outputId": "f342557f-d8dd-499d-8639-c290578695d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-26 03:40:14.465663: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-26 03:40:15.345913: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-26 03:40:15.346015: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-26 03:40:15.346032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-md==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.10.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.22.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v0BhOwwPyZPJ"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "def preprocessing(sentence):\n",
        "    \n",
        "    # Clear the html tag by using regular expression.\n",
        "    sentence = re.sub(\"<[^>]*>\", \"\", sentence) # RegEx to remove html tags\n",
        "    sentence = re.sub(\"[^\\x00-\\x7F]+\", \"\", sentence) # RegEx to remove another languages that is not English.\n",
        "    stopwords = list(STOP_WORDS)\n",
        "    doc = nlp(sentence)\n",
        "    cleaned_tokens = []\n",
        "    \n",
        "    # Cleaned its with spacy, remove symbol, punct and empty space\n",
        "    # It's smart enough to not remove the . in the middle of sentences. E.g. \"np.array\"\n",
        "\n",
        "    for token in doc: \n",
        "        if token.text not in stopwords and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and \\\n",
        "            token.pos_ != 'SYM' and token.pos_!= 'X':\n",
        "                cleaned_tokens.append(token.lemma_.lower().strip())\n",
        "                \n",
        "    return \" \".join(cleaned_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYjH2XYkyZPK"
      },
      "source": [
        "## Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BMxZQmaDyZPK"
      },
      "outputs": [],
      "source": [
        "# Create tokenizer\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        text = preprocessing(text) \n",
        "        yield tokenizer(text)\n",
        "\n",
        "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "train_size = 90000 # take limited samples for HW limitation\n",
        "tokenized_dataset_train = yield_tokens(train_split[:train_size])\n",
        "tokenized_dataset_test = yield_tokens(test_split[:30000])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numericalizing"
      ],
      "metadata": {
        "id": "gy3PYcnsJB3M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZXNumv2yZPL",
        "outputId": "daf38e07-770d-4e8b-e23e-4313bbdb80c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8711\n",
            "['<unk>', '<eos>', '\"', ',', ')', '=', ':', '#', \"'\", 'the']\n"
          ]
        }
      ],
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_split[:train_size]), min_freq=5) \n",
        "vocab.insert_token('<unk>', 0)           \n",
        "vocab.insert_token('<eos>', 1)            \n",
        "vocab.set_default_index(vocab['<unk>'])   \n",
        "print(len(vocab))                         \n",
        "print(vocab.get_itos()[:10])       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KASlbGm1yZPL"
      },
      "source": [
        "# 3. Prepare the Batch Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "tags": [],
        "id": "1X3S-YJsyZPL"
      },
      "outputs": [],
      "source": [
        "def get_data(dataset, vocab, batch_size):\n",
        "    data = []                                                   \n",
        "    for example in dataset:       \n",
        "        #appends eos so we know it ends....so model learn how to end...                             \n",
        "        tokens = example.append('<eos>') #end of sentence\n",
        "        #numericalize          \n",
        "        tokens = [vocab[token] for token in example] \n",
        "        data.extend(tokens)                                    \n",
        "    data = torch.LongTensor(data)                                 \n",
        "    num_batches = data.shape[0] // batch_size \n",
        "    data = data[:num_batches * batch_size]                       \n",
        "    data = data.view(batch_size, num_batches)        \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NwOMhuGsyZPM"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "train_data = get_data(tokenized_dataset_train, vocab, batch_size)\n",
        "valid_data = get_data(tokenized_dataset_test, vocab, batch_size)\n",
        "# test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape, valid_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G0IsssVMpHs",
        "outputId": "1fb5b360-e353-4550-e3f1-c66028a9d731"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([128, 3353]), torch.Size([128, 1139]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NhYsvIL_yZPM"
      },
      "outputs": [],
      "source": [
        "# Save data for later use\n",
        "import pickle\n",
        "\n",
        "# Save preprocessed data\n",
        "object_data = train_data\n",
        "file_train = open('train_data.pkl', 'wb') \n",
        "pickle.dump(object_data, file_train)\n",
        "\n",
        "object_data = valid_data\n",
        "file_val = open('valid_data.pkl', 'wb') \n",
        "pickle.dump(object_data, file_val)\n",
        "\n",
        "# Save vocab\n",
        "object_data = vocab\n",
        "file_vocab = open('vocab.pkl', 'wb') \n",
        "pickle.dump(object_data, file_vocab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvMzGRs_yZPN"
      },
      "source": [
        "# 4.  Modeling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "tags": [],
        "id": "stlEnysAyZPN"
      },
      "outputs": [],
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
        "                \n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size,emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n",
        "                                        dropout = dropout_rate, batch_first = True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        #when you do LM, you look forward, so it does not make sense to do bidirectional\n",
        "        self.fc = nn.Linear(hid_dim,vocab_size)\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        #this function gonna be run in the beginning of the epoch\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "\n",
        "        return hidden, cell #return as tuple\n",
        "\n",
        "    def detach_hidden(self, hidden):\n",
        "        hidden, cell = hidden\n",
        "        hidden = hidden.detach() #removing this hidden from gradients graph\n",
        "        cell =  cell.detach() #removing this hidden from gradients graph\n",
        "        return hidden, cell\n",
        "\n",
        "    def forward(self, src, hidden):\n",
        "        #src: [batch_size, seq_len]\n",
        "\n",
        "        #embed \n",
        "        embedded = self.embedding(src)\n",
        "        #embed : [batch_size, seq_len, emb_dim]\n",
        "\n",
        "        #send this to the lstm\n",
        "        #we want to put hidden here... because we want to reset hidden .....\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        #output : [batch_size, seq_len, hid_dim] ==> all hidden states\n",
        "        #hidden : [batch_size, seq_len, hid_dim] ==> last hidden states from each layer\n",
        "\n",
        "        output = self.dropout(output)\n",
        "        prediction = self.fc(output)\n",
        "        #prediction: [batch size, seq_len, vocab_size]\n",
        "        return prediction, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVpO7NsfyZPO"
      },
      "source": [
        "# 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XoprvPRSyZPO"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "emb_dim = 1024                # 400 in the paper\n",
        "hid_dim = 1024                # 1150 in the paper\n",
        "num_layers = 2                # 3 in the paper\n",
        "dropout_rate = 0.65              \n",
        "lr = 1e-3                     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WH8QQ-QyZPO",
        "outputId": "717fb399-abe5-404a-a31e-e5b02b1bf30e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 34,642,439 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LHiADwooyZPO"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, seq_len, idx):\n",
        "    src    = data[:, idx:idx+seq_len]                   \n",
        "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
        "    return src, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GhcPGOlCyZPP"
      },
      "outputs": [],
      "source": [
        "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    # drop all batches that are not a multiple of seq_len\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    \n",
        "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        hidden = model.detach_hidden(hidden)\n",
        "\n",
        "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
        "        src, target = src.to(device), target.to(device)\n",
        "        batch_size = src.shape[0]\n",
        "        prediction, hidden = model(src, hidden)               \n",
        "\n",
        "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
        "        target = target.reshape(-1)\n",
        "        loss = criterion(prediction, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) #prevent gradient explosion - clip is basically \n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5EAMOY9FyZPP"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, num_batches - 1, seq_len):\n",
        "            hidden = model.detach_hidden(hidden)\n",
        "            src, target = get_batch(data, seq_len, idx)\n",
        "            src, target = src.to(device), target.to(device)\n",
        "            batch_size= src.shape[0]\n",
        "\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "            target = target.reshape(-1)\n",
        "\n",
        "            loss = criterion(prediction, target)\n",
        "            epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YVlb61tyZPP",
        "outputId": "de5f2438-a257-43f0-88dc-0aea8bce39a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 160.870\n",
            "\tValid Perplexity: 75.836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 67.319\n",
            "\tValid Perplexity: 60.222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 53.260\n",
            "\tValid Perplexity: 54.701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 45.978\n",
            "\tValid Perplexity: 50.609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 40.485\n",
            "\tValid Perplexity: 48.519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 36.005\n",
            "\tValid Perplexity: 46.522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 32.022\n",
            "\tValid Perplexity: 44.858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 28.784\n",
            "\tValid Perplexity: 43.920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 25.898\n",
            "\tValid Perplexity: 43.203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 23.482\n",
            "\tValid Perplexity: 42.684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 21.493\n",
            "\tValid Perplexity: 43.761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 19.305\n",
            "\tValid Perplexity: 41.887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 17.666\n",
            "\tValid Perplexity: 42.552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 16.315\n",
            "\tValid Perplexity: 43.592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 15.406\n",
            "\tValid Perplexity: 43.752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 14.858\n",
            "\tValid Perplexity: 44.273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 14.599\n",
            "\tValid Perplexity: 44.605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 14.474\n",
            "\tValid Perplexity: 44.751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 14.425\n",
            "\tValid Perplexity: 44.769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Perplexity: 14.358\n",
            "\tValid Perplexity: 44.778\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 20\n",
        "seq_len  = 50\n",
        "clip    = 0.25\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_data, optimizer, criterion, \n",
        "                batch_size, seq_len, clip, device)\n",
        "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
        "                seq_len, device)\n",
        "\n",
        "    lr_scheduler.step(valid_loss)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-val-auto.pt')\n",
        "\n",
        "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Testing"
      ],
      "metadata": {
        "id": "-C-Ksr-vPuNK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7im1fxdyZPP"
      },
      "source": [
        "# 7. Real World Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ueVbUCCHyZPQ"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt)\n",
        "    indices = [vocab[t] for t in tokens]\n",
        "    batch_size = 1\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(max_seq_len):\n",
        "            src = torch.LongTensor([indices]).to(device)\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            \n",
        "            #prediction: [batch size, seq len, vocab size]\n",
        "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
        "            \n",
        "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
        "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
        "            \n",
        "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
        "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
        "                break\n",
        "\n",
        "            indices.append(prediction) #autoregressive, thus output becomes input\n",
        "\n",
        "    itos = vocab.get_itos()\n",
        "    tokens = [itos[i] for i in indices]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiKWmKGcyZPQ",
        "outputId": "e5ed9e8a-3cf4-4c47-b61d-38e63a5ff7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n",
            "from tensorflow import library\n",
            "\n",
            "0.7\n",
            "from tensorflow import numpy\n",
            "\n",
            "0.75\n",
            "from tensorflow import numpy\n",
            "\n",
            "0.8\n",
            "from tensorflow import numpy\n",
            "\n",
            "1.0\n",
            "from tensorflow import util\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = 'from tensorflow import'\n",
        "max_seq_len = 30\n",
        "seed = 0\n",
        "            #superdiverse   more diverse\n",
        "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0] \n",
        "#sample from this distribution higher probability will get more change\n",
        "for temperature in temperatures:\n",
        "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
        "                          vocab, device, seed)\n",
        "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J5uKCUEs9rhV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}