{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8547e1-d2a8-43cb-894a-3545c8b96cb6",
   "metadata": {},
   "source": [
    "## Assignment 3 (Dependency Parser)\n",
    "\n",
    "Everything should be done ON MY code, no new code.\n",
    "\n",
    "1. Read https://aclanthology.org/D14-1082.pdf and maybe just write one paragraph summary in your README.md in your github\n",
    "\n",
    "2. Do something called ablation study (meaning try to delete something so we know the impact of that deleted thing - very common in NLP)\n",
    "- Recall that we have 18 word + 18 pos + 12 dep features\n",
    "- Try to delete only the 12 dep features and check UAS\n",
    "- Try to delete only the 18 pos features and check UAS\n",
    "3. Do another comparison study testing the embedding\n",
    "- Chaky uses some embedding\n",
    "- Try to use (1) glove embedding (smallest), (2) nn.Embedding (train from scratch) and compare with Chaky's embedding - on how it affects the UAS\n",
    "4. Do some testing, compare 2-3 sentences with spaCy and see whether our neural network gives the same dependency.\n",
    "\n",
    "Criteria:\n",
    "0: not done\n",
    "1: ok\n",
    "2: with comments/explanation like how Chaky does his tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e681b6a7-8b82-480f-88a9-f32dec5f749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = \"Md. Sakib Bin Alam\"\n",
    "ID = \"st122574\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218f5bc-3521-4e90-8f10-5e6e12b3bdee",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c85b8d5-3ccc-46a8-ab08-22f94ede1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm  #gimmick for progressbar when you train\n",
    "import pickle #saving and loading models\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb07cf-caea-41aa-ac6d-4add6d6014d0",
   "metadata": {},
   "source": [
    "## 1. Parsing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a12f7e0-df92-43f5-a82c-84724981d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basically, it takes the current state of the buffer, stack, dependencies\n",
    "#tell us how SHIFT, LA, RA changes these three objects\n",
    "\n",
    "class Parsing(object):\n",
    "    \n",
    "    #init stack, buffer, dep\n",
    "    def __init__(self, sentence):  \n",
    "        self.sentence = sentence     #['The', 'cat', 'sat]  #conll format which is already in the tokenized form\n",
    "        self.stack    = ['ROOT']\n",
    "        self.buffer   = sentence[:]  #in the beginning, everything is inside the buffer\n",
    "        self.dep      = []           #maintains a list of tuples of dep\n",
    "    \n",
    "    #parse function that tells me how shift, la, ra changes these three objects\n",
    "    def parse_step(self, transition):     #transition could be either S, LA, RA\n",
    "        if transition == 'S':\n",
    "            #get the top guy in the buffer and put in stack\n",
    "            head = self.buffer.pop(0)\n",
    "            self.stack.append(head)\n",
    "        elif transition == 'LA':  #stack = [ROOT, He, has] ==> append to dep (has, he) and then He is gone from the stack [ROOT, has]\n",
    "            dependent = self.stack.pop(-2)  #He\n",
    "            self.dep.append((self.stack[-1], dependent))  #(has, he)\n",
    "        elif transition == 'RA':\n",
    "            #can you guys try to this???\n",
    "            dependent = self.stack.pop()  #stack = [ROOT, has, control] ==> dep (has, control), control will be gone fromt he stack [ROOT, has]\n",
    "            self.dep.append((self.stack[-1], dependent))\n",
    "        else:\n",
    "            print(f\"Bad transition: {transition}\")\n",
    "    \n",
    "    #given some series of transition, it gonna for-loop the parse function\n",
    "    def parse(self, transitions):\n",
    "        for t in transitions:\n",
    "            self.parse_step(t)\n",
    "        return self.dep\n",
    "    \n",
    "    #check whether things are finished - no need to do anymore functions....\n",
    "    def is_completed(self):\n",
    "        return (len(self.buffer) == 0) and (len(self.stack) == 1)  #so buffer is empty and ROOT is the only guy in stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99afbf6-cbed-48d4-91cc-ed63c702ec64",
   "metadata": {},
   "source": [
    "### Minibatch parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb617195-9efe-48a7-bc2e-177e2559b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    dep = []  #all the resulting dep\n",
    "    \n",
    "    #init Parsing instance for each sentence in the batch\n",
    "    partial_parses = [Parsing(sentence) for sentence in sentences]  #in tokenized form\n",
    "    #Parsing(['The', 'cat', 'sat']), Parsing(['Chaky', 'is', 'mad'])\n",
    "    \n",
    "    unfinished_parses = partial_parses[:]\n",
    "    \n",
    "    #while we still have sentence\n",
    "    while unfinished_parses:  #if there are still a Parsing object\n",
    "    \n",
    "        #take a certain batch of sentence\n",
    "        minibatch = unfinished_parses[:batch_size] #number of Parsing object\n",
    "        \n",
    "        #create a dummy model to tell us what's the next transition for each sentence\n",
    "        transitions = model.predict(minibatch) \n",
    "        #transitions = [S, S, .....]\n",
    "        #minibatch   = [Parsing(sentence1), Parsing(sentence2)]\n",
    "        \n",
    "                \n",
    "        # for transition predicted this dummy model\n",
    "        for transition, partial_parse in zip(transitions, minibatch):\n",
    "            #parse step\n",
    "            #transition: S\n",
    "            #partial_parse: Parsing(sentence)\n",
    "            partial_parse.parse_step(transition)\n",
    "            \n",
    "        #remove any sentence is finish\n",
    "        unfinished_parses[:] = [p for p in unfinished_parses if not p.is_completed()]\n",
    "    \n",
    "    dep = [parse.dep for parse in partial_parses]\n",
    "    \n",
    "    return dep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d62202-39a3-4ca1-9d93-9bfbe97adf1a",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5763b721-d6b7-4d02-bc45-97b063f68a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(filename):\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        i = 0\n",
    "        word, pos, head, dep = [], [], [], []\n",
    "        for line in f.readlines():\n",
    "            i = i+1\n",
    "            wa = line.strip().split('\\t')  #['1', 'In', '_', 'ADP', 'IN', '_', '5', 'case', '_', '_']\n",
    "            #In <--------  5th guy\n",
    "            #     case\n",
    "            \n",
    "            if len(wa) == 10:  #if all the columns are there\n",
    "                word.append(wa[1].lower())\n",
    "                pos.append(wa[4])\n",
    "                head.append(int(wa[6]))\n",
    "                dep.append(wa[7])\n",
    "            \n",
    "            #the row is not exactly 10, it means new sentence\n",
    "            elif len(word) > 0:  #if there is somethign inside the word\n",
    "                examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  #in the sentence level\n",
    "                word, pos, head, dep = [], [], [], [] #clear word, pos, head, dep\n",
    "        \n",
    "        if len(word) > 0:  #if there is somethign inside the word\n",
    "            examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})  #in the sentence level\n",
    "\n",
    "    return examples        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5863903a-390f-467c-99cc-9d61b8479f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"Loading data...\")\n",
    "    train_set = read_conll(\"train.conll\")\n",
    "    dev_set   = read_conll(\"dev.conll\")\n",
    "    test_set   = read_conll(\"test.conll\")\n",
    "    \n",
    "    #make the dataset smaller so that machine can handle quickly\n",
    "    train_set = train_set[:1000]\n",
    "    dev_set   = dev_set[:500]\n",
    "    test_set  = test_set[:500]\n",
    "    \n",
    "    return train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11ad5a-8dab-4d8e-963e-6092afbae737",
   "metadata": {},
   "source": [
    "# Part I: Ablation study\n",
    "- Try to delete only the 12 dep features and check UAS\n",
    "- Try to delete only the 18 pos features and check UAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b756a4-9282-4b69-a11a-4a521ef3d515",
   "metadata": {},
   "source": [
    "## 3. Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5531bfd-fc5a-4ec6-995f-cd0956cea270",
   "metadata": {},
   "source": [
    "- create a tok2id dictionary in the __init__ function\n",
    "- numercalize numercalize the words, dependencies, and pos tags\n",
    "- create training data, create_instances by leveraging the ground truth of the dependencies\n",
    "- finally the parse function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1ede5a18-5bef-4450-b469-55b41f2cce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified Parser class to ensure dep features and pos features\n",
    "\n",
    "P_PREFIX = '<p>:' #indicating pos tags\n",
    "D_PREFIX = '<d>:' #indicating dependency tags\n",
    "UNK      = '<UNK>'\n",
    "NULL     = '<NULL>'\n",
    "ROOT     = '<ROOT>'\n",
    "\n",
    "class Parser(object):\n",
    "\n",
    "    def __init__(self, dataset, pos_feat=True, dep_feat=True):\n",
    "        \n",
    "        self.n_features = 18\n",
    "        self.pos_feat = pos_feat\n",
    "        self.dep_feat = dep_feat\n",
    "\n",
    "        tok2id = dict() \n",
    "        trans = ['L', 'R', 'S']\n",
    "        self.n_trans = len(trans)\n",
    "        self.n_deprel = 1\n",
    "        \n",
    "        # consider dep features\n",
    "        if self.dep_feat == True:\n",
    "            self.root_dep = 'root'\n",
    "                \n",
    "        #get all the dep of the dataset as list, e.g., ['root', 'acl', 'nmod', 'nmod:npmod']\n",
    "         # Basically, if it's false we ill not create this list\n",
    "            all_dep = [self.root_dep] + list(set([w for ex in dataset\n",
    "                                                for w in ex['dep']\n",
    "                                                if w != self.root_dep]))\n",
    "        \n",
    "        #1. put dep into tok2id lookup table, with D_PREFIX so we know it is dependency\n",
    "        #{'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'D_PREFIX:<NULL>': 30}\n",
    "            tok2id = {D_PREFIX + l: i for (i, l) in enumerate(all_dep)}\n",
    "            tok2id[D_PREFIX + NULL] = self.D_NULL = len(tok2id)\n",
    "        \n",
    "        #we are using \"unlabeled\" where we do not label with the dependency\n",
    "        #thus the number of dependency relation is 1\n",
    "            trans = ['L', 'R', 'S']\n",
    "            self.n_deprel = 1   #because we are not predicting the relations, we are only predicting S, L, R\n",
    "        \n",
    "            #create a simple lookup table mapping action and id\n",
    "            #e.g., tran2id: {'L': 0, 'R': 1, 'S': 2}\n",
    "            #e.g., id2tran: {0: 'L', 1: 'R', 2: 'S'}\n",
    "            self.n_trans = len(trans)\n",
    "            self.tran2id = {t: i for (i, t) in enumerate(trans)}  #use for easy coding\n",
    "            self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
    "\n",
    "        # consider pos features\n",
    "        if self.pos_feat == True:\n",
    "            #2. put pos tags into tok2id lookup table, with P_PREFIX so we know it is pos\n",
    "            tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
    "                                    offset=len(tok2id)))\n",
    "            tok2id[P_PREFIX + UNK]  = self.P_UNK  = len(tok2id)  #also remember the pos tags of unknown\n",
    "            tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
    "            tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
    "            \n",
    "            #now tok2id:  {'P_PREFIX:root': 0, 'P_PREFIX:acl': 1, ..., 'P_PREFIX:JJR': 62, 'P_PREFIX:<UNK>': 63, 'P_PREFIX:<NULL>': 64, 'P_PREFIX:<ROOT>': 65}\n",
    "        \n",
    "        #3. put word into tok2id lookup table\n",
    "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[UNK]  = self.UNK = len(tok2id)\n",
    "        tok2id[NULL] = self.NULL = len(tok2id)\n",
    "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
    "        \n",
    "        #now tok2id: {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'memory': 340, 'mr.': 341, '<UNK>': 342, '<NULL>': 343, '<ROOT>': 344}\n",
    "        \n",
    "        #create id2tok\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
    "        \n",
    "        # If some of the feature are not use, the number of features also change, so we need to fix this!\n",
    "        if self.pos_feat == True:\n",
    "            self.n_features = self.n_features + 18\n",
    "        if self.dep_feat == True:\n",
    "            self.n_features = self.n_features + 12\n",
    "        \n",
    "        self.n_tokens = len(tok2id)\n",
    "        \n",
    "    #utility function, in case we want to convert token to id\n",
    "    #function to turn train set with words to train set with id instead using tok2id\n",
    "    def numericalize(self, examples):\n",
    "        numer_examples = []\n",
    "        for ex in examples:\n",
    "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
    "                                  else self.UNK for w in ex['word']]\n",
    "            if self.pos_feat == True:\n",
    "                pos  = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
    "                                    else self.P_UNK for w in ex['pos']]\n",
    "            head = [-1] + ex['head']\n",
    "            if self.dep_feat == True:\n",
    "                dep  = [-1] + [self.tok2id[D_PREFIX + w] if D_PREFIX + w in self.tok2id\n",
    "                                else -1 for w in ex['dep']]\n",
    "            \n",
    "            if (self.pos_feat == False) and (self.dep_feat == False):\n",
    "                numer_examples.append({'word': word, 'head': head})\n",
    "\n",
    "            elif (self.pos_feat == False) and (self.dep_feat == True):\n",
    "                numer_examples.append({'word': word, 'head': head, 'dep': dep})\n",
    "\n",
    "            elif (self.pos_feat == True) and (self.dep_feat == False):\n",
    "                numer_examples.append({'word': word, 'pos': pos,\n",
    "                        'head': head})\n",
    "            else:\n",
    "                numer_examples.append({'word': word, 'pos': pos,\n",
    "                                 'head': head, 'dep': dep})\n",
    "        return numer_examples\n",
    "    \n",
    "    #function to extract features to form a feature embedding matrix\n",
    "    def extract_features(self, stack, buf, arcs, ex):\n",
    "             \n",
    "        #ex['word']:  [55, 32, 33, 34, 35, 30], i.e., ['root', 'ms.', 'haag', 'plays', 'elianti', '.']\n",
    "        #ex['pos']:   [29, 14, 14, 16, 14, 17], i.e., ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
    "        #ex['head']:  [-1, 2, 3, 0, 3, 3]  or ['root', 'compound', 'nsubj', 'root', 'dobj', 'punct']}\n",
    "        #ex['dep']:   [-1, 1, 2, 0, 6, 12] or ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
    "\n",
    "        #stack     :  [0]\n",
    "        #buffer    :  [1, 2, 3, 4, 5]\n",
    "        \n",
    "        if stack[0] == \"ROOT\":\n",
    "            stack[0] = 0  #start the stack with [ROOT]\n",
    "        \n",
    "        p_features = [] #pos features (2a, 2b, 2c) - 18\n",
    "        d_features = [] #dep features (3b, 3c) - 12\n",
    "        \n",
    "        #last 3 things on the stack as features\n",
    "        #if the stack is less than 3, then we simply append NULL from the left\n",
    "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
    "        \n",
    "        # next 3 things on the buffer as features\n",
    "        #if the buffer is less than 3, simply append NULL\n",
    "        #the reason why NULL is appended on end because buffer is read left to right\n",
    "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
    "        \n",
    "        if self.pos_feat == True:\n",
    "            #corresponding pos tags\n",
    "            p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
    "            p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
    "        \n",
    "        #get leftmost children based on the dependency arcs\n",
    "        def get_lc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
    "\n",
    "        #get right most children based on the dependency arcs\n",
    "        def get_rc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
    "                          reverse=True)\n",
    "\n",
    "        #get the leftmost and rightmost children of the top two words, thus we loop 2 times\n",
    "        for i in range(2):\n",
    "            if i < len(stack):\n",
    "                k = stack[-i-1] #-1, -2 last two in the stack\n",
    "                \n",
    "                #the first and second lefmost/rightmost children of the top two words (i=1, 2) on the stack\n",
    "                lc = get_lc(k)  \n",
    "                rc = get_rc(k)\n",
    "                \n",
    "                #the leftmost of leftmost/rightmost of rightmost children of the top two words on the stack:\n",
    "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
    "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
    "\n",
    "                #(leftmost of first word on stack, rightmost of first word, \n",
    "                # leftmost of the second word on stack, rightmost of second, \n",
    "                # leftmost of leftmost, rightmost of rightmost\n",
    "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
    "\n",
    "                if self.pos_feat == True:\n",
    "                    #corresponding pos\n",
    "                    p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
    "                    p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
    "\n",
    "                if self.dep_feat == True:\n",
    "                    #corresponding dep\n",
    "                    d_features.append(ex['dep'][lc[0]] if len(lc) > 0 else self.D_NULL)\n",
    "                    d_features.append(ex['dep'][rc[0]] if len(rc) > 0 else self.D_NULL)\n",
    "                    d_features.append(ex['dep'][lc[1]] if len(lc) > 1 else self.D_NULL)\n",
    "                    d_features.append(ex['dep'][rc[1]] if len(rc) > 1 else self.D_NULL)\n",
    "                    d_features.append(ex['dep'][llc[0]] if len(llc) > 0 else self.D_NULL)\n",
    "                    d_features.append(ex['dep'][rrc[0]] if len(rrc) > 0 else self.D_NULL)\n",
    "                \n",
    "            else:\n",
    "                #attach NULL when they don't exist\n",
    "                features += [self.NULL] * 6\n",
    "                if self.pos_feat == True:\n",
    "                    p_features += [self.P_NULL] * 6\n",
    "                if self.dep_feat == True:\n",
    "                    d_features += [self.D_NULL] * 6\n",
    "\n",
    "        if (self.pos_feat == True) and (self.dep_feat == True):\n",
    "            features += p_features + d_features\n",
    "\n",
    "        elif (self.pos_feat == False) and (self.dep_feat == True):\n",
    "            features += d_features\n",
    "\n",
    "        elif (self.pos_feat == True) and (self.dep_feat == False):\n",
    "            features += p_features\n",
    "        \n",
    "        else:\n",
    "            features = features\n",
    "\n",
    "        assert len(features) == self.n_features  #assert they are 18 + 18 + 12\n",
    "        \n",
    "        return features\n",
    "\n",
    "    #generate training examples\n",
    "    #from the training sentences and their gold parse trees \n",
    "    def create_instances(self, examples):  #examples = word, pos, head, dep\n",
    "        all_instances = []\n",
    "        \n",
    "        for i, ex in enumerate(examples):\n",
    "            #Ms. Haag plays Elianti .\n",
    "            #e.g., ex['word]: [344, 163, 99, 164, 165, 68]\n",
    "            #here 344 stands for ROOT\n",
    "            #Chaky - I cheated and take a look\n",
    "            n_words = len(ex['word']) - 1  #excluding the root\n",
    "            \n",
    "            #arcs = {(head, tail, dependency label)}\n",
    "            stack = [0]\n",
    "            buf = [i + 1 for i in range(n_words)]  #[1, 2, 3, 4, 5]\n",
    "            arcs = []\n",
    "            instances = []\n",
    "            \n",
    "            #because that's the maximum number of shift, leftarcs, rightarcs you can have\n",
    "            #this will determine the sample size of each training example\n",
    "            #if given five words, we will get a sample of (10, 48) where 10 comes from 5 * 2, and 48 is n_features\n",
    "            #but this for loop can be break if there is nothing left....\n",
    "            \n",
    "            for i in range(n_words * 2):  #maximum times you can do either S, L, R\n",
    "                \n",
    "                #get the gold transition based on the parse trees\n",
    "                #gold_t can be either shift(2), leftarc(0), or rightarc(1)\n",
    "                gold_t = self.get_oracle(stack, buf, ex)\n",
    "                \n",
    "                #if gold_t is None, no need to extract features.....\n",
    "                if gold_t is None:\n",
    "                    break\n",
    "                \n",
    "                #make sure when the model predicts, we inform the current state of stack and buffer, so\n",
    "                #the model is not allowed to make any illegal action, e.g., buffer is empty but trying to pop\n",
    "                legal_labels = self.legal_labels(stack, buf)                \n",
    "                assert legal_labels[gold_t] == 1\n",
    "                \n",
    "                #extract the features that we want to use.\n",
    "                features = self.extract_features(stack, buf, arcs, ex)\n",
    "                instances.append((features, legal_labels, gold_t))\n",
    "                \n",
    "                #shift \n",
    "                if gold_t == 2:\n",
    "                    stack.append(buf[0])\n",
    "                    buf = buf[1:]\n",
    "                #left arc \n",
    "                elif gold_t == 0:\n",
    "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
    "                    stack = stack[:-2] + [stack[-1]]\n",
    "                #right arc\n",
    "                else:\n",
    "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
    "                    stack = stack[:-1]\n",
    "                    \n",
    "            else:\n",
    "                all_instances += instances\n",
    "\n",
    "        if self.dep_feat == False: # We only need the word and pos\n",
    "            all_instances = [[instance[0], instance[2]] for instance in all_instances]\n",
    "\n",
    "        return all_instances\n",
    "    \n",
    "    #provide an one hot encoding of the labels\n",
    "    def legal_labels(self, stack, buf):\n",
    "        labels =  ([1] if len(stack) > 2  else [0]) * self.n_deprel  #left arc but you cannot do ROOT <--- He\n",
    "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel  #right arc because ROOT --> He\n",
    "        labels += [1] if len(buf) > 0 else [0]  #shift\n",
    "        return labels\n",
    "    \n",
    "    #a simple function to check punctuation POS tags\n",
    "    def punct(self, pos):\n",
    "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
    "    \n",
    "    #decide whether to shift, leftarc, or rightarc, based on gold parse trees\n",
    "    #this is needed to create training examples which contain samples and ground truth\n",
    "    def get_oracle(self, stack, buf, ex):\n",
    "        \n",
    "        #leave if the stack is only 1, thus nothing to predict....\n",
    "        if len(stack) < 2:\n",
    "            return self.n_trans - 1\n",
    "        \n",
    "        #predict based on the last two words on the stack\n",
    "        #stack: [ROOT, he, has]\n",
    "        i0 = stack[-1] #has\n",
    "        i1 = stack[-2] #he\n",
    "        \n",
    "        #get the head and dependency\n",
    "        h0 = ex['head'][i0]\n",
    "        h1 = ex['head'][i1]\n",
    "\n",
    "        if self.dep_feat == True:\n",
    "            d0 = ex['dep'][i0]\n",
    "            d1 = ex['dep'][i1]\n",
    "        \n",
    "        #either shift, left arc or right arc\n",
    "        #\"Shift\" = 2; \"LA\" = 0; \"RA\" = 1\n",
    "        #if head of the second last word is the last word, then leftarc\n",
    "        if (i1 > 0) and (h1 == i0):\n",
    "            return 0  #action is left arc ---> gold_t\n",
    "        #if head of the last word is the second last word, then rightarc\n",
    "        #make sure nothing in the buffer has head with the last word on the stack\n",
    "        #otherwise, we lose the last word.....\n",
    "        elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "            return 1  #right arc\n",
    "        #otherwise shift, if something is left in buffer, otherwise, do nothing....\n",
    "        else:\n",
    "            return None if len(buf) == 0 else 2  #shift\n",
    "        \n",
    "    def parse(self, dataset, eval_batch_size=5000):\n",
    "        sentences = []\n",
    "        sentence_id_to_idx = {}\n",
    "        \n",
    "        for i, example in enumerate(dataset):\n",
    "            \n",
    "            #example['word']=[188, 186, 186, ..., 59]\n",
    "            #n_words=37\n",
    "            #sentence=[1, 2, 3, 4, 5,.., 37]\n",
    "            \n",
    "            n_words = len(example['word']) - 1\n",
    "            sentence = [j + 1 for j in range(n_words)]            \n",
    "            sentences.append(sentence)\n",
    "            \n",
    "            #mapping the object unique id to the i            \n",
    "            #The id is the object's memory address\n",
    "            sentence_id_to_idx[id(sentence)] = i\n",
    "            \n",
    "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
    "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
    "        \n",
    "        UAS = all_tokens = 0.0\n",
    "        with tqdm(total=len(dataset)) as prog:\n",
    "            for i, ex in enumerate(dataset):\n",
    "                head = [-1] * len(ex['word'])\n",
    "                for h, t, in dependencies[i]:\n",
    "                    head[t] = h\n",
    "                \n",
    "                if (self.pos_feat == True) and (self.dep_feat == True):\n",
    "                    for pred_h, gold_h, gold_l, pos in \\\n",
    "                            zip(head[1:], ex['head'][1:], ex['dep'][1:], ex['pos'][1:]):\n",
    "                            assert self.id2tok[pos].startswith(P_PREFIX)\n",
    "                            pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
    "                            if (not self.punct(pos_str)):\n",
    "                                UAS += 1 if pred_h == gold_h else 0\n",
    "                                all_tokens += 1\n",
    "\n",
    "                elif (self.pos_feat == False) and (self.dep_feat == True):\n",
    "                    for pred_h, gold_h, gold_l in \\\n",
    "                            zip(head[1:], ex['head'][1:], ex['dep'][1:]):\n",
    "                            UAS += 1 if pred_h == gold_h else 0\n",
    "                            all_tokens += 1\n",
    "\n",
    "                elif (self.pos_feat == True) and (self.dep_feat == False):\n",
    "                    for pred_h, gold_h, pos in \\\n",
    "                            zip(head[1:], ex['head'][1:], ex['pos'][1:]):\n",
    "                            assert self.id2tok[pos].startswith(P_PREFIX)\n",
    "                            pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
    "                            if (not self.punct(pos_str)):\n",
    "                                UAS += 1 if pred_h == gold_h else 0\n",
    "                                all_tokens += 1\n",
    "\n",
    "                prog.update(i + 1)\n",
    "        UAS /= all_tokens\n",
    "        return UAS, dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc02951e-8712-427c-acc2-df845897b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(object):\n",
    "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
    "        self.parser = parser\n",
    "        self.dataset = dataset\n",
    "        self.sentence_id_to_idx = sentence_id_to_idx\n",
    "\n",
    "    def predict(self, partial_parses):\n",
    "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dep,\n",
    "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
    "                for p in partial_parses]\n",
    "        mb_x = np.array(mb_x).astype('int32')\n",
    "        mb_x = torch.from_numpy(mb_x).long()\n",
    "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
    "\n",
    "        pred = self.parser.model(mb_x)\n",
    "        pred = pred.detach().numpy()\n",
    "        \n",
    "        #we need to multiply 10000 with legal labels, to force the model not to make any impossible prediction\n",
    "        #other, when we parse sequentially, sometimes there is nothing in the buffer or stack, thus error....        \n",
    "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
    "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4151c938-1821-482a-bc89-1f46e1c00c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a simple function to create ids.....\n",
    "def build_dict(keys, offset=0):\n",
    "    #keys = ['P_PREFIX:IN', 'P_PREFIX:DT', 'P_PREFIX:NNP', 'P_PREFIX:CD', so on...]\n",
    "    #offset is needed because this tok2id has something already inside....\n",
    "    count = Counter()\n",
    "    for key in keys:\n",
    "        count[key] += 1\n",
    "    \n",
    "    #most_common = [('P_PREFIX:NN', 70), ('P_PREFIX:IN', 57), ... , ('P_PREFIX:JJR', 1)]\n",
    "    #we use most_common in case we only want some maximum pos tags....\n",
    "    mc = count.most_common()\n",
    "    \n",
    "    #{'P_PREFIX:NN': 31, 'P_PREFIX:IN': 32, .., 'P_PREFIX:JJR': 62} \n",
    "    return {w[0]: index + offset for (index, w) in enumerate(mc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b5e6f-a0b0-434c-b6d3-c88e1790553f",
   "metadata": {},
   "source": [
    "## 4. Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9f34fac-1248-4c7c-9574-b76dba9f67bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"4. Loading pretrained embeddings...\",)\n",
    "# start = time.time()\n",
    "# word_vectors = {}\n",
    "# for line in open(\"en-cw.txt\").readlines():\n",
    "#     we = line.strip().split() #we = word embeddings - first column: word;  the rest is embedding\n",
    "#     word_vectors[we[0]] = [float(x) for x in we[1:]] #{word: [list of 50 numbers], nextword: [another list], so on...}\n",
    "    \n",
    "# #create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "# #we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "# embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "# for token in parser.tok2id:\n",
    "#         i = parser.tok2id[token]\n",
    "#         if token in word_vectors:\n",
    "#             embeddings_matrix[i] = word_vectors[token]\n",
    "#         elif token.lower() in word_vectors:\n",
    "#             embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "# print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "# print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a858333-2466-4b5b-bcb9-c6cd7ea081a8",
   "metadata": {},
   "source": [
    "## 5. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b252cc8-670f-44f6-83c4-6ca537c1fced",
   "metadata": {},
   "source": [
    "- Testing the parser\n",
    "- Testing the numericalize\n",
    "- Preprocessing (create instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb2b5148-97d8-4104-be0f-71968dff120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. word, pos_features, and dep_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2b0473a-d19f-45da-81ad-f43f02359346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "2. Building parser\n",
      "took 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "# Testing the parser\n",
    "\n",
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "print(\"2. Building parser\")\n",
    "start = time.time()\n",
    "parser = Parser(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecc7f295-2ef5-48a0-8297-d95c704076ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  ['ms.', 'haag', 'plays', 'elianti', '.']\n",
      "Pos:  ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
      "Head:  [2, 3, 0, 3, 3]\n",
      "Dep:  ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
      "Word:  [5156, 304, 1364, 1002, 2144, 87]\n",
      "Pos:  [84, 42, 42, 55, 42, 46]\n",
      "Head:  [-1, 2, 3, 0, 3, 3]\n",
      "Dep:  [-1, 7, 11, 0, 10, 29]\n"
     ]
    }
   ],
   "source": [
    "# Testing the numericalize\n",
    "\n",
    "#before numericalize\n",
    "print(\"Word: \", train_set[1][\"word\"])\n",
    "print(\"Pos: \",  train_set[1][\"pos\"])\n",
    "print(\"Head: \", train_set[1][\"head\"])\n",
    "print(\"Dep: \",  train_set[1][\"dep\"])\n",
    "\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "\n",
    "#after numericalize\n",
    "print(\"Word: \", train_set[1][\"word\"])\n",
    "print(\"Pos: \",  train_set[1][\"pos\"])\n",
    "print(\"Head: \", train_set[1][\"head\"])\n",
    "print(\"Dep: \",  train_set[1][\"dep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eba69b47-1ea0-4d2d-b182-7f8cf9579fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Preprocessing training data...\n",
      "took 1.70 seconds\n",
      "([5155, 5155, 5156, 91, 113, 806, 5155, 5155, 5155, 5155, 5155, 5155, 5155, 5155, 5155, 5155, 5155, 5155, 83, 83, 84, 40, 41, 42, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38], [0, 0, 1], 2)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "print(train_examples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ac0b9-caa1-4144-88b2-57d31454bb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18485234-6642-4eb4-b0ea-db0834924bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word and dep_features (will not consider pos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85798bb7-7eba-4a50-b0cd-d1fd264ca6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "2. Building parser\n",
      "took 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# Testing the parser\n",
    "\n",
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "print(\"2. Building parser\")\n",
    "start = time.time()\n",
    "parser = Parser(train_set, pos_t = False)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a295bc07-5114-4835-9e89-b0116c039eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  ['ms.', 'haag', 'plays', 'elianti', '.']\n",
      "Pos:  ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
      "Head:  [2, 3, 0, 3, 3]\n",
      "Dep:  ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
      "Word:  [5110, 258, 1318, 956, 2098, 41]\n",
      "pos feature is not considered\n",
      "Head:  [-1, 2, 3, 0, 3, 3]\n",
      "Dep:  [-1, 7, 11, 0, 10, 29]\n"
     ]
    }
   ],
   "source": [
    "# Testing the numericalize\n",
    "\n",
    "#before numericalize\n",
    "print(\"Word: \", train_set[1][\"word\"])\n",
    "print(\"Pos: \",  train_set[1][\"pos\"])\n",
    "print(\"Head: \", train_set[1][\"head\"])\n",
    "print(\"Dep: \",  train_set[1][\"dep\"])\n",
    "\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "\n",
    "#after numericalize\n",
    "print(\"Word: \", train_set[1][\"word\"])\n",
    "try:\n",
    "    print(\"Pos: \",  train_set[1][\"pos\"])\n",
    "except:\n",
    "    print('pos feature is not considered')\n",
    "print(\"Head: \", train_set[1][\"head\"])\n",
    "print(\"Dep: \",  train_set[1][\"dep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f94cec57-03cc-4431-98e5-84728590efa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Preprocessing training data...\n",
      "took 1.43 seconds\n",
      "([5109, 5109, 5110, 45, 67, 760, 5109, 5109, 5109, 5109, 5109, 5109, 5109, 5109, 5109, 5109, 5109, 5109, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38], [0, 0, 1], 2)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "print(train_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531fda9f-4e2f-46a9-bd2a-436baa54f6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "2. Building parser\n",
      "took 0.02 seconds\n",
      "Word:  ['ms.', 'haag', 'plays', 'elianti', '.']\n",
      "Pos:  ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
      "Head:  [2, 3, 0, 3, 3]\n",
      "Dep:  ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
      "Word:  [5117, 265, 1325, 963, 2105, 48]\n",
      "Pos:  [45, 3, 3, 16, 3, 7]\n",
      "Head:  [-1, 2, 3, 0, 3, 3]\n",
      "DEP feature False\n",
      "5. Preprocessing training data...\n",
      "took 1.60 seconds\n",
      "[[5116, 5116, 5117, 52, 74, 767, 5116, 5116, 5116, 5116, 5116, 5116, 5116, 5116, 5116, 5116, 5116, 5116, 44, 44, 45, 1, 2, 3, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44], 2]\n"
     ]
    }
   ],
   "source": [
    "# dep_t = False\n",
    "\n",
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "print(\"2. Building parser\")\n",
    "start = time.time()\n",
    "parser = Parser(train_set, dep_t=False)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "#before numericalize\n",
    "print(\"Word: \", train_set[1][\"word\"])\n",
    "print(\"Pos: \",  train_set[1][\"pos\"])\n",
    "print(\"Head: \", train_set[1][\"head\"])\n",
    "print(\"Dep: \",  train_set[1][\"dep\"])\n",
    "\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "\n",
    "#after numericalize\n",
    "print(\"Word: \", train_set[1][\"word\"])\n",
    "print(\"Pos: \",  train_set[1][\"pos\"])\n",
    "print(\"Head: \", train_set[1][\"head\"])\n",
    "try:\n",
    "    print(\"Dep: \",  train_set[1][\"dep\"])\n",
    "except:\n",
    "    print(\"DEP feature False\")\n",
    "\n",
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "print(train_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b6850-81d3-4a10-abf8-bbc71e059290",
   "metadata": {},
   "source": [
    "## 6. Minibatch Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b24b50c-4bbb-4592-b929-f93a20c864d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(data, minibatch_size, shuffle=True):\n",
    "    data_size = len(data[0])\n",
    "    indices = np.arange(data_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
    "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
    "        yield [_minibatch(d, minibatch_indices) for d in data]\n",
    "\n",
    "def _minibatch(data, minibatch_idx):\n",
    "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
    "\n",
    "# def minibatches(data, batch_size):\n",
    "#     x = np.array([d[0] for d in data])\n",
    "#     y = np.array([d[2] for d in data])\n",
    "#     one_hot = np.zeros((y.size, 3))\n",
    "#     one_hot[np.arange(y.size), y] = 1\n",
    "#     return get_minibatches([x, one_hot], batch_size)\n",
    "\n",
    "# update the minibatches function so that it works without dep features\n",
    "\n",
    "def minibatches_update(data, batch_size, dep_t=True):\n",
    "\n",
    "    if dep_t == True:\n",
    "        x = np.array([d[0] for d in data])\n",
    "        y = np.array([d[2] for d in data])\n",
    "        #print(x.shape)\n",
    "        #print(y.shape)\n",
    "        one_hot = np.zeros((y.size, 3))\n",
    "        #print(one_hot.shape)\n",
    "        one_hot[np.arange(y.size), y] = 1\n",
    "    \n",
    "    if dep_t == False:\n",
    "        x = np.array([d[0] for d in data])\n",
    "        y = np.array([d[1] for d in data])\n",
    "        #print(x.shape)\n",
    "        #print(y.shape)\n",
    "        one_hot = np.zeros((y.size, 3))\n",
    "        #print(one_hot.shape)\n",
    "        one_hot[np.arange(y.size), y] = 1\n",
    "\n",
    "    return get_minibatches([x, one_hot], batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa8e866-ceea-4fce-9860-edca6aa26a47",
   "metadata": {},
   "source": [
    "## 7. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c8692a0-5b82-47d5-b6ae-435aa7387ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, n_features=48,\n",
    "                 hidden_size=400, n_classes=3, dropout_prob=0.5):\n",
    "\n",
    "        super(ParserModel, self).__init__()\n",
    "        self.n_features   = n_features\n",
    "        self.n_classes    = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size   = embeddings.shape[1]\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
    "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "\n",
    "        self.embed_to_hidden = nn.Linear(n_features * self.embed_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.hidden_to_logits = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    def embedding_lookup(self, t):\n",
    "        #t:  batch_size, n_features\n",
    "        batch_size = t.size()[0]\n",
    "                    \n",
    "        x = self.pretrained_embeddings(t)        \n",
    "        x = x.reshape(-1, self.n_features * self.embed_size)\n",
    "        # x = (1024, 48 * 50)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: (1024, 48)\n",
    "        embeddings = self.embedding_lookup(t)  \n",
    "    \n",
    "        # embeddings: (1024, 48 * 50)\n",
    "        hidden = self.embed_to_hidden(embeddings)\n",
    "    \n",
    "        # hidden: (1024, 200)\n",
    "        hidden_activations = F.relu(hidden)\n",
    "        # hidden_activations: (1024, 200)\n",
    "        thin_net = self.dropout(hidden_activations)\n",
    "        # thin_net: (1024, 200)\n",
    "        logits = self.hidden_to_logits(thin_net)\n",
    "        # logits: (1024, 3)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae8e3c2b-e26c-4630-8306-89d1dbcef873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a class to get the average.....\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b85dee9c-9843-4b86-8a71-7a46750a47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005, dep_t=True):\n",
    "    \n",
    "    best_dev_UAS = 0\n",
    "    \n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=0.001)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "        dev_UAS = train_for_epoch(\n",
    "            parser, train_data, dev_data, optimizer, loss_func, batch_size, dep_t)\n",
    "        print(f'Current UAS scores = {dev_UAS}')\n",
    "        if dev_UAS > best_dev_UAS:\n",
    "            best_dev_UAS = dev_UAS\n",
    "            print(\"New best dev UAS! Saving model.\")\n",
    "            torch.save(parser.model.state_dict(), output_path)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size, dep_t=True):\n",
    "    \n",
    "    parser.model.train()  # Places model in \"train\" mode, i.e. apply dropout layer\n",
    "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    with tqdm(total=(n_minibatches)) as prog:\n",
    "        if dep_t == True:\n",
    "            for i, (train_x, train_y) in enumerate(minibatches_update(train_data, batch_size)):\n",
    "                \n",
    "                #train_x:  batch_size, n_features\n",
    "                #train_y:  batch_size, target(=3)\n",
    "                \n",
    "                optimizer.zero_grad() \n",
    "                loss = 0.\n",
    "                train_x = torch.from_numpy(train_x).long()  #long() for int so embedding works....\n",
    "                train_y = torch.from_numpy(train_y.nonzero()[1]).long()  #get the index with 1 because torch expects label to be single integer\n",
    "\n",
    "                # Forward pass: compute predicted logits.\n",
    "                logits = parser.model(train_x)\n",
    "                # Compute loss\n",
    "                #print(logits.shape, train_y.shape)\n",
    "                loss = loss_func(logits, train_y)\n",
    "                # Compute gradients of the loss w.r.t model parameters.\n",
    "                loss.backward()\n",
    "                # Take step with optimizer.\n",
    "                optimizer.step()\n",
    "\n",
    "                prog.update(1)\n",
    "                loss_meter.update(loss.item())\n",
    "        else:\n",
    "            for i, (train_x, train_y) in enumerate(minibatches_update(train_data, batch_size, dep_t=False)):\n",
    "            \n",
    "                #train_x:  batch_size, n_features\n",
    "                #train_y:  batch_size, target(=3)\n",
    "                \n",
    "                optimizer.zero_grad() \n",
    "                loss = 0.\n",
    "                train_x = torch.from_numpy(train_x).long()  #long() for int so embedding works....\n",
    "                train_y = torch.from_numpy(train_y.nonzero()[1]).long()  #get the index with 1 because torch expects label to be single integer\n",
    "\n",
    "                # Forward pass: compute predicted logits.\n",
    "                logits = parser.model(train_x)\n",
    "                # Compute loss\n",
    "                #print(logits.shape, train_y.shape)\n",
    "                loss = loss_func(logits, train_y)\n",
    "                # Compute gradients of the loss w.r.t model parameters.\n",
    "                loss.backward()\n",
    "                # Take step with optimizer.\n",
    "                optimizer.step()\n",
    "\n",
    "                prog.update(1)\n",
    "                loss_meter.update(loss.item())\n",
    "\n",
    "    print(\"Average Train Loss: {}\".format(loss_meter.avg))\n",
    "    print(\"Evaluating on dev set\",)\n",
    "    parser.model.eval()  # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
    "        \n",
    "    dev_UAS, _ = parser.parse(dev_data)\n",
    "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "    return dev_UAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f3c88-f30f-4158-9202-eba80208d358",
   "metadata": {},
   "source": [
    "## 8. Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a4896-0b3d-4a84-9ba7-2f60b436b1e9",
   "metadata": {},
   "source": [
    "### Training with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f618ed2-66d7-4aad-afa0-e2f3ec4653e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building parser\n",
      "took 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "print(\"Building parser\")\n",
    "start = time.time()\n",
    "parser = Parser(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eadddbde-b637-449f-ad97-00bb0a5dfdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Loading pretrained embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
      "took 2.78 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"4. Loading pretrained embeddings...\",)\n",
    "start = time.time()\n",
    "word_vectors = {}\n",
    "for line in open(\"en-cw.txt\").readlines():\n",
    "    we = line.strip().split() #we = word embeddings - first column: word;  the rest is embedding\n",
    "    word_vectors[we[0]] = [float(x) for x in we[1:]] #{word: [list of 50 numbers], nextword: [another list], so on...}\n",
    "    \n",
    "#create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "#we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87d451bf-0b21-4fcd-a1e7-df4522a6ced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "took 1.93 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29586d0c-d839-474d-b906-a8536727470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 25.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.5864982747783264\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 4743316.89it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 57.82\n",
      "Current UAS scores = 0.5781842304776346\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 26.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3039866512020429\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6578548.59it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 62.46\n",
      "Current UAS scores = 0.6246209249431387\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 26.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.24606077590336403\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6570320.88it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 64.45\n",
      "Current UAS scores = 0.6445223654283548\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 27.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.2091195105264584\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6614413.66it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 66.82\n",
      "Current UAS scores = 0.6682145564821834\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 26.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.18472907847414413\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6645203.67it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 70.08\n",
      "Current UAS scores = 0.7008150113722517\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 26.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1663073223705093\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6575255.03it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 71.91\n",
      "Current UAS scores = 0.7191053828658074\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 26.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.14720060272763172\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6390644.93it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 73.41\n",
      "Current UAS scores = 0.7340788476118272\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 24.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.13324525471155843\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 5058122.24it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 72.22\n",
      "Current UAS scores = 0.7222327520849128\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 26.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.12297150477146108\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 5950934.28it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 73.82\n",
      "Current UAS scores = 0.7381539044730857\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 24.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.11471605145682891\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6033358.32it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 74.11\n",
      "Current UAS scores = 0.7410917361637605\n",
      "New best dev UAS! Saving model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create directory if it does not exist for saving the weights...\n",
    "output_dir = \"output/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
    "output_path = output_dir + \"model.weights_all\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel(embeddings_matrix)\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266cb6a-c384-49ff-a4b8-772e0c712e29",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "013a1121-f2f5-49f5-8602-250b3d4e0339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing [pos_feature False]...\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6567117.64it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 73.93\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"=\")\n",
    "print(\"Testing [pos_feature False]...\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n",
    "UAS, dependencies = parser.parse(test_set)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb18de-9e8a-46fd-8c47-fbb22b2e2cfe",
   "metadata": {},
   "source": [
    "### Training with dep features (pos_feature = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ad2c89e-f6f4-4f62-8b88-880e011d1fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building parser\n",
      "took 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "print(\"Building parser\")\n",
    "start = time.time()\n",
    "parser = Parser(train_set, pos_t=False)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68973605-b9ee-49f9-9481-d32b5ae7bdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Loading pretrained embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5111, 50)\n",
      "took 2.54 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"4. Loading pretrained embeddings...\",)\n",
    "start = time.time()\n",
    "word_vectors = {}\n",
    "for line in open(\"en-cw.txt\").readlines():\n",
    "    we = line.strip().split() #we = word embeddings - first column: word;  the rest is embedding\n",
    "    word_vectors[we[0]] = [float(x) for x in we[1:]] #{word: [list of 50 numbers], nextword: [another list], so on...}\n",
    "    \n",
    "#create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "#we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4f631a5-3891-445a-a84d-68f2acfa5556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "took 1.54 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74ca697d-9355-4d67-83f1-3204417a750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 37.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.574447072421511\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 18011333.90it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 51.40\n",
      "Current UAS scores = 0.5140381571273848\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 39.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3279167804867029\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 17065801.77it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 56.99\n",
      "Current UAS scores = 0.569940848121303\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 39.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.26491370145231485\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 17875279.05it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 59.77\n",
      "Current UAS scores = 0.5976839123552445\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 37.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.22907536259541908\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 18607182.23it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 62.93\n",
      "Current UAS scores = 0.6293426643339165\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 38.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.2030701640372475\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 18609818.84it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 63.91\n",
      "Current UAS scores = 0.6390902274431393\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 39.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.18215366111447415\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 17860086.22it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 65.22\n",
      "Current UAS scores = 0.6521702907606431\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 40.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.15877805184572935\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 18695251.81it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 67.07\n",
      "Current UAS scores = 0.6706656669166042\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 39.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.14535739257310828\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 18695251.81it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 68.35\n",
      "Current UAS scores = 0.683495792718487\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 38.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.13197186418498555\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 12136969.23it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 69.03\n",
      "Current UAS scores = 0.6903274181454636\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 36.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.11791128277157743\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 16815613.33it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 68.39\n",
      "Current UAS scores = 0.6839123552445222\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create directory if it does not exist for saving the weights...\n",
    "output_dir = \"output/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
    "output_path = output_dir + \"model.weights_dep\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel(embeddings_matrix, n_features=30) # since pos_feature = False\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375c384-6631-426c-bc8c-9e9055610dbe",
   "metadata": {},
   "source": [
    "### Testing with Dep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da980274-04d2-4465-8fdc-9b01bd89113d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing [pos_feature False]...\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 15469274.91it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 71.17\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"=\")\n",
    "print(\"Testing [pos_feature False]...\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n",
    "UAS, dependencies = parser.parse(test_set)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741603c4-8ff5-4d2c-b78b-cb7878947e5a",
   "metadata": {},
   "source": [
    "### Training with pos features (dep_feature = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9210848-375a-49b4-9daa-3b5c2c04a080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building parser\n",
      "took 0.03 seconds\n",
      "4. Loading pretrained embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5118, 50)\n",
      "took 2.72 seconds\n",
      "5. Preprocessing training data...\n",
      "took 1.71 seconds\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "print(\"Building parser\")\n",
    "start = time.time()\n",
    "parser = Parser(train_set, dep_t=False)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "\n",
    "print(\"4. Loading pretrained embeddings...\",)\n",
    "start = time.time()\n",
    "word_vectors = {}\n",
    "for line in open(\"en-cw.txt\").readlines():\n",
    "    we = line.strip().split() #we = word embeddings - first column: word;  the rest is embedding\n",
    "    word_vectors[we[0]] = [float(x) for x in we[1:]] #{word: [list of 50 numbers], nextword: [another list], so on...}\n",
    "    \n",
    "#create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "#we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bc87ca6-8769-4128-8c7e-404a9f3885ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 31.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.4769222792237997\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6426921.65it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 63.91\n",
      "Current UAS scores = 0.6391205458680819\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 31.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.25726959730188054\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 4634968.29it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 69.67\n",
      "Current UAS scores = 0.6967399545109931\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 31.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.20340208491931358\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6878564.10it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 72.37\n",
      "Current UAS scores = 0.7237490523123579\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 31.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.16982805542647839\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6735688.79it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 73.65\n",
      "Current UAS scores = 0.7365428354814253\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 33.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.14566293886552253\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6840230.93it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 75.16\n",
      "Current UAS scores = 0.7516110689916603\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 34.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.12573518666128317\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6872894.66it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 77.30\n",
      "Current UAS scores = 0.7730288097043214\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 34.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.11079300381243229\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6694402.94it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 76.90\n",
      "Current UAS scores = 0.7689537528430629\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 32.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.0990099251891176\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6494215.52it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 77.45\n",
      "Current UAS scores = 0.7745451099317665\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 31.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.08840357729544242\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6883791.86it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 77.81\n",
      "Current UAS scores = 0.7781463229719484\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 33.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.07978511873322229\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6717171.85it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 78.13\n",
      "Current UAS scores = 0.7812736921910538\n",
      "New best dev UAS! Saving model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create directory if it does not exist for saving the weights...\n",
    "output_dir = \"output_pos/{:%Y%m%d_%H%M%S}\".format(datetime.now())\n",
    "output_path = output_dir + \"model.weights_pos\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel(embeddings_matrix, n_features=36) # Because now we only use 2 features (18 + 18)\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005, dep_t=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ce5da-5027-424f-815b-bd1ee42585b2",
   "metadata": {},
   "source": [
    "### Testing with POS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64d5bf5a-fc26-465b-b5b2-7e86f25fcae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing [pos_feature False]...\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6847274.26it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 78.93\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"=\")\n",
    "print(\"Testing [pos_feature False]...\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n",
    "UAS, dependencies = parser.parse(test_set)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0890c12-38f2-42a0-ade8-a13824c5b420",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacea5c5-5c4f-4140-952a-2c7a28244ec2",
   "metadata": {},
   "source": [
    "| Features         | Dev UAS | Test UAS |\n",
    "|----------------: |---------|:--------:|\n",
    "|  Words, POS, DEP | 73.91   | 74.11    |\n",
    "|  Words, DEP      | 69.13   | 71.17    |\n",
    "|  Words, POS      | 78.13   | 78.93    |\n",
    "\n",
    "So, we find that words + pos is giving the best score. Therefore, pos is very significant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9782bf-b291-4e05-b404-43f28b00a919",
   "metadata": {},
   "source": [
    "# Part II: Embedding Comparison\n",
    "- Chaky uses some embedding\n",
    "- Try to use (1) glove embedding (smallest), (2) nn.Embedding (train from scratch) and compare with Chaky's embedding - on how it affects the UAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e64c2d-cb3c-42b2-9f65-0d46e390ab57",
   "metadata": {},
   "source": [
    "***I will select dep_feature = False since it's giving the best result.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb3ff50-a73a-4dd7-95cc-4c433ddad55f",
   "metadata": {},
   "source": [
    "## Glove (pre-trained model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e500337f-5f2f-4447-9a97-ecacffd3c2cc",
   "metadata": {},
   "source": [
    "## Training with Pos features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "45593db8-967a-4fe1-8be3-01066282912e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building parser\n",
      "took 0.03 seconds\n",
      "Loading pretrained Glove embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5118, 50)\n",
      "took 8.06 seconds\n",
      "5. Preprocessing training data...\n",
      "took 1.94 seconds\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "print(\"Building parser\")\n",
    "start = time.time()\n",
    "parser = Parser(train_set, dep_t=False)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "\n",
    "print(\"Loading pretrained Glove embeddings...\")\n",
    "start = time.time()\n",
    "word_vectors = {}\n",
    "for line in open(\"glove.6B.50d.txt\", encoding=\"utf8\").readlines():\n",
    "    we = line.strip().split() #we = word embeddings - first column: word;  the rest is embedding\n",
    "    word_vectors[we[0]] = [float(x) for x in we[1:]] #{word: [list of 50 numbers], nextword: [another list], so on...}\n",
    "    \n",
    "#create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "#we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d30e4a0f-3cc3-4db5-92f6-79dd0abab275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 32.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.5826965409020582\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6753960.76it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 52.63\n",
      "Current UAS scores = 0.5262509476876421\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 33.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3380398179093997\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 4802945.53it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 62.71\n",
      "Current UAS scores = 0.6270849128127369\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 32.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.2675399808213115\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6729217.81it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 67.85\n",
      "Current UAS scores = 0.6785443517816527\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 32.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.22660683250675598\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6718116.76it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 69.84\n",
      "Current UAS scores = 0.6983510235026535\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 32.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1979390773922205\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6601862.12it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 70.75\n",
      "Current UAS scores = 0.7075435936315391\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 33.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.17428600043058395\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6213691.83it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 73.47\n",
      "Current UAS scores = 0.7347422289613343\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 32.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.15344157411406437\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6428415.90it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 73.90\n",
      "Current UAS scores = 0.7390068233510235\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 32.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1366265956312418\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6511360.63it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 75.18\n",
      "Current UAS scores = 0.751800606520091\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 31.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.12632450057814518\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6658933.43it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 76.18\n",
      "Current UAS scores = 0.7618460955269143\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 31.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.10859201460455854\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 4787756.45it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 76.04\n",
      "Current UAS scores = 0.7604245640636846\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create directory if it does not exist for saving the weights...\n",
    "output_dir = \"output_pos/{:%Y%m%d_%H%M%S}\".format(datetime.now())\n",
    "output_path = output_dir + \"glove_weights\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel(embeddings_matrix, n_features=36) # Because now we only use 2 features (18 + 18)\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005, dep_t=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97961c45-2b2a-46a3-8053-909b0ed16660",
   "metadata": {},
   "source": [
    "### Testing with Pos features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9ef78bf6-edd3-408e-9c01-34a11d347816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing [pos_feature False]...\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6212075.35it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 77.45\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"=\")\n",
    "print(\"Testing [pos_feature False]...\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n",
    "UAS, dependencies = parser.parse(test_set)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b880647-1c6d-4762-846d-03166a0321f8",
   "metadata": {},
   "source": [
    "## Embedding train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aace2de0-c605-45d9-bb27-783977693040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserModel2(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, n_features=48,\n",
    "                 hidden_size=400, n_classes=3, dropout_prob=0.5):\n",
    "\n",
    "        super(ParserModel2, self).__init__()\n",
    "        self.n_features   = n_features\n",
    "        self.n_classes    = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size   = embeddings.shape[1]\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.embeddings = nn.Embedding(embeddings.shape[0], self.embed_size) # Modified\n",
    "        \n",
    "        self.embed_to_hidden = nn.Linear(n_features * self.embed_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.hidden_to_logits = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    def embedding_lookup(self, t):\n",
    "        #t:  batch_size, n_features\n",
    "        batch_size = t.size()[0]\n",
    "                    \n",
    "        x = self.embeddings(t) # Modified\n",
    "        x = x.reshape(-1, self.n_features * self.embed_size)\n",
    "        # x = (1024, 48 * 50)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: (1024, 48)\n",
    "        embeddings = self.embedding_lookup(t)  \n",
    "    \n",
    "        # embeddings: (1024, 48 * 50)\n",
    "        hidden = self.embed_to_hidden(embeddings)\n",
    "    \n",
    "        # hidden: (1024, 200)\n",
    "        hidden_activations = F.relu(hidden)\n",
    "        # hidden_activations: (1024, 200)\n",
    "        thin_net = self.dropout(hidden_activations)\n",
    "        # thin_net: (1024, 200)\n",
    "        logits = self.hidden_to_logits(thin_net)\n",
    "        # logits: (1024, 3)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "22e9e2d2-fbb3-46ca-84ad-444cf90add5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 33.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.5773063705613216\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6833379.85it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 52.93\n",
      "Current UAS scores = 0.5292835481425322\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 35.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.32659017605086166\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6695000.14it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 61.57\n",
      "Current UAS scores = 0.6157126611068992\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 34.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.26428445025036734\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6745288.72it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 64.14\n",
      "Current UAS scores = 0.6413949962092494\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 34.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.22924170239518085\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6772768.69it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 65.24\n",
      "Current UAS scores = 0.6523881728582259\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 33.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.20145549159497023\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6702773.50it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 69.45\n",
      "Current UAS scores = 0.6944655041698257\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 34.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.17374896382292113\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6805495.01it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 69.89\n",
      "Current UAS scores = 0.6989196360879454\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 34.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.15637375600636005\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6832046.82it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 71.83\n",
      "Current UAS scores = 0.7183472327520849\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 34.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.14135734116037688\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6743556.98it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 71.72\n",
      "Current UAS scores = 0.7172100075815011\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 34.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.12547871579105654\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6764658.00it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 74.47\n",
      "Current UAS scores = 0.7446929492039424\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 33.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.11315890510256092\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6759174.70it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 73.77\n",
      "Current UAS scores = 0.7376800606520091\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create directory if it does not exist for saving the weights...\n",
    "output_dir = \"output_pos/{:%Y%m%d_%H%M%S}\".format(datetime.now())\n",
    "output_path = output_dir + \"embedding_scractch_weights\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel2(embeddings_matrix, n_features=36) # Because now we only use 2 features (18 + 18)\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005, dep_t=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a13ee-5792-4e16-b802-207a75ab7002",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6c1af75e-6b2f-44a9-bdde-474a0bcb6aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Testing [pos_feature False]...\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6774340.74it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 74.32\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"=\")\n",
    "print(\"Testing [pos_feature False]...\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n",
    "UAS, dependencies = parser.parse(test_set)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07705477-0fe2-4705-aa4a-b6e0478ecb6f",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aa82f9-782c-43c5-9c11-7cc63f116fad",
   "metadata": {},
   "source": [
    "| Embedding case   | Dev UAS | Test UAS |\n",
    "|----------------: |---------|:--------:|\n",
    "|  Glove           | 76.18   | 77.45    |\n",
    "|  Embedding scratch| 74.47  | 74.12    |\n",
    "\n",
    "So, we see that Glove embedding is giving the better result. Interestingly embedding from scratch also giving a very good result. Ultimately by comparing all in (words + pos features) is giving the best result, that was done in previous analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05c98d-ceb0-4217-a8aa-c4348e501fa6",
   "metadata": {},
   "source": [
    "# Part IV: 4. Do some testing, compare 2-3 sentences with spaCy and see whether our neural network gives the same dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3282f6fb-2970-42a0-ba75-a66523539f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "_, _, test_set = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "441350ed-becd-4685-a082-97dc8adee698",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "\n",
    "\n",
    "for sent in test_set:\n",
    "    if len(sent['word']) <= 10:\n",
    "        test_sentences.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5ef94505-c88e-44cf-af98-5c9975a63a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ca132479-38b9-4877-a39b-7e78dd6aa7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_test_sentences = test_sentences[:3]\n",
    "len(three_test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9c641876-d42e-41c3-80a5-1e5821263aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#three_test_sentence\n",
    "final_sentences = []\n",
    "\n",
    "for sent in three_test_sentences:\n",
    "    sentence_join = ' '.join(sent['word'])\n",
    "    final_sentences.append(sentence_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7466138a-3594-491f-8457-a739b6e728fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"no , it was n't black monday .\",\n",
       " 'the finger-pointing has already begun .',\n",
       " '`` the equity market was illiquid .']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0cd2d-b1e3-4976-b342-4bd756b16181",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "77bfda0e-72e9-4130-917e-9d5b54c8aa09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"270758e8ea194c1a81c712528c2f6ccc-0\" class=\"displacy\" width=\"1450\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">no</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">it</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">n't</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">black</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">monday</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-270758e8ea194c1a81c712528c2f6ccc-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-270758e8ea194c1a81c712528c2f6ccc-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">intj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-270758e8ea194c1a81c712528c2f6ccc-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-270758e8ea194c1a81c712528c2f6ccc-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-270758e8ea194c1a81c712528c2f6ccc-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-270758e8ea194c1a81c712528c2f6ccc-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-270758e8ea194c1a81c712528c2f6ccc-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-270758e8ea194c1a81c712528c2f6ccc-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-270758e8ea194c1a81c712528c2f6ccc-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-270758e8ea194c1a81c712528c2f6ccc-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-270758e8ea194c1a81c712528c2f6ccc-0-5\" stroke-width=\"2px\" d=\"M595,264.5 C595,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-270758e8ea194c1a81c712528c2f6ccc-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-270758e8ea194c1a81c712528c2f6ccc-0-6\" stroke-width=\"2px\" d=\"M595,264.5 C595,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-270758e8ea194c1a81c712528c2f6ccc-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"5ab3f65ed76f46f884441fba73b5c1d7-0\" class=\"displacy\" width=\"1450\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">finger</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">pointing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">has</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">already</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">begun</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,2.0 1100.0,2.0 1100.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-5ab3f65ed76f46f884441fba73b5c1d7-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,266.5 L1273.0,254.5 1257.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7a00ca11590c41cea4ea29dc8b78df42-0\" class=\"displacy\" width=\"1450\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">`</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">`</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">equity</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">market</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">illiquid</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7a00ca11590c41cea4ea29dc8b78df42-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 925.0,2.0 925.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7a00ca11590c41cea4ea29dc8b78df42-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7a00ca11590c41cea4ea29dc8b78df42-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,89.5 920.0,89.5 920.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7a00ca11590c41cea4ea29dc8b78df42-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7a00ca11590c41cea4ea29dc8b78df42-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,177.0 740.0,177.0 740.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7a00ca11590c41cea4ea29dc8b78df42-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7a00ca11590c41cea4ea29dc8b78df42-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7a00ca11590c41cea4ea29dc8b78df42-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7a00ca11590c41cea4ea29dc8b78df42-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7a00ca11590c41cea4ea29dc8b78df42-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7a00ca11590c41cea4ea29dc8b78df42-0-5\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7a00ca11590c41cea4ea29dc8b78df42-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7a00ca11590c41cea4ea29dc8b78df42-0-6\" stroke-width=\"2px\" d=\"M945,352.0 C945,177.0 1265.0,177.0 1265.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7a00ca11590c41cea4ea29dc8b78df42-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,354.0 L1273.0,342.0 1257.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy #displacy is for visualization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for sent in final_sentences:\n",
    "    doc = nlp(sent)\n",
    "    options = {\"collapse_punct\": False}\n",
    "    displacy.render(doc, options = options, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7e7ba6-4ccc-4bed-8ddc-a09cb7c24223",
   "metadata": {},
   "source": [
    "## Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1f52601f-9a34-4078-975c-8e9302556015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 32.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.6023708507418633\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6792296.34it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 55.66\n",
      "Current UAS scores = 0.5565769522365428\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 33.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3332404761264722\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6706709.77it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 61.95\n",
      "Current UAS scores = 0.6195034116755117\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 34.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.2728568213060498\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6912778.16it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 63.56\n",
      "Current UAS scores = 0.6356141015921153\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 34.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.23963302467018366\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6068484.61it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 67.95\n",
      "Current UAS scores = 0.6794920394238059\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 34.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.2045258299137155\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 4627129.99it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 70.13\n",
      "Current UAS scores = 0.7012888551933283\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 32.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.18433721487720808\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6685372.56it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 70.31\n",
      "Current UAS scores = 0.7030894617134192\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 34.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.16536891801903644\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6783262.87it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 73.55\n",
      "Current UAS scores = 0.7355003790750568\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 35.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1479460272627572\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6558672.83it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 74.24\n",
      "Current UAS scores = 0.7424184988627748\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 33.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1362463398836553\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6772681.37it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 74.37\n",
      "Current UAS scores = 0.7436504927975739\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 48/48 [00:01<00:00, 33.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.12194157112389803\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6149969.87it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 75.44\n",
      "Current UAS scores = 0.7543593631539045\n",
      "New best dev UAS! Saving model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ParserModel2(embeddings_matrix, n_features=36)\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005, dep_t=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d97c587e-13ee-4c93-9543-d3f4917adcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1081it [00:00, 995836.29it/s]         \n"
     ]
    }
   ],
   "source": [
    "numericalized_sentences = parser.numericalize(test_sentences)\n",
    "\n",
    "UAS, dependencies = parser.parse(numericalized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "df27edef-e589-4f65-9c21-7573ce3e66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a5d84721-6b93-4f5d-b1ab-2e8681975b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 5)\n",
      "(6, 4)\n",
      "(6, 3)\n",
      "(6, 2)\n",
      "(6, 1)\n",
      "(6, 7)\n",
      "(6, 8)\n",
      "(0, 6)\n"
     ]
    }
   ],
   "source": [
    "for row in range(len(dependencies)):\n",
    "    word_obj_list = []\n",
    "    arcs_obj_list = []\n",
    "    \n",
    "    for col in range(len(dependencies[row])):\n",
    "        print(dependencies[row][col])\n",
    "        left, right = dependencies[row][col] #Tuple Pair\n",
    "        word = final_sentences[row][col]\n",
    "        word_obj = {\"text\": \"It\", \"tag\": \"DT\"}\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "db340cd4-a569-43e0-823f-8244546ee972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"53d1cd583f364aef825538d7cfdf6214-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">It</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DT</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VBZ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DT</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">pen</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53d1cd583f364aef825538d7cfdf6214-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53d1cd583f364aef825538d7cfdf6214-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\"></textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53d1cd583f364aef825538d7cfdf6214-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53d1cd583f364aef825538d7cfdf6214-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\"></textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-53d1cd583f364aef825538d7cfdf6214-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-53d1cd583f364aef825538d7cfdf6214-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\"></textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ex = {\n",
    "    \"words\": [\n",
    "        {\"text\": \"It\", \"tag\": \"DT\"},\n",
    "        {\"text\": \"was\", \"tag\": \"VBZ\"},\n",
    "        {\"text\": \"a\", \"tag\": \"DT\"},\n",
    "        {\"text\": \"pen\", \"tag\": \"NN\"}\n",
    "    ],\n",
    "    \"arcs\": [\n",
    "        {\"start\": 0, \"end\": 1, \"label\": \"\", \"dir\": \"left\"},\n",
    "        {\"start\": 2, \"end\": 3, \"label\": \"\", \"dir\": \"left\"},\n",
    "        {\"start\": 1, \"end\": 3, \"label\": \"\", \"dir\": \"right\"}\n",
    "    ]\n",
    "}\n",
    "displacy.render(ex, style=\"dep\", manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22726ad1-3de0-4f12-9427-bc16d2fe2ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
