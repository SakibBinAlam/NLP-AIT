{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b0f8046b-7234-499e-a4cb-003e2457556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = \"Md. Sakib Bin Alam\"\n",
    "ID = \"st122574\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400a81c-070e-462b-9c04-04830d461d9d",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "\n",
    "Constraint: Only use our code (not other code....)\n",
    "\n",
    "1. I guess you already try a bigger corpus\n",
    "2. I guess you already try window size 2\n",
    "3. I guess you already have skipgram, skipgram(neg), cbow, glove\n",
    "\n",
    "Do this:\n",
    "1. Compare them based on syntactic accuracy and semantic accuracy, similar to how is done in https://nlp.stanford.edu/pubs/glove.pdf (see Table 2) - NO NEED to try 1000 or 300 embed size.....I just want you to learn how to do experiment.....\n",
    "2. Try to find a correlation with just ONE similarity dataset (which humans judge how similar is two words.....)\n",
    "\n",
    "Point criteria:\n",
    "0:  Not done\n",
    "1: ok\n",
    "2: with comments / explanation / figures just like how Chaky explain thing....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffdcc498-9f55-47c7-8e53-96048069d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0fdb9-0ffd-4c81-9594-147ba9d69145",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1170ff1-000a-470c-82e3-3f5138be6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "# #import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fd89c7-6690-458d-95d9-90da5efe7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('questions-words.txt') as file:\n",
    "    data = [line.strip() for line in file.readlines() if line[0] != ':']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447b9393-00d1-478a-9529-9c73b4f9bf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 19544\n",
      "['Athens Greece Baghdad Iraq', 'Athens Greece Bangkok Thailand', 'Athens Greece Beijing China', 'Athens Greece Berlin Germany', 'Athens Greece Bern Switzerland', 'Athens Greece Cairo Egypt', 'Athens Greece Canberra Australia', 'Athens Greece Hanoi Vietnam', 'Athens Greece Havana Cuba', 'Athens Greece Helsinki Finland']\n"
     ]
    }
   ],
   "source": [
    "print(\"length:\", len(data))\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca3cb4-7ead-4ad2-8bb9-70e1aa5c8ff1",
   "metadata": {},
   "source": [
    "## 2. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d56ee01b-2d3b-42fc-9460-3a54508ba4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in data:\n",
    "#     print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9338f821-6c6a-4b1f-9eb3-35031b12b23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Athens', 'Greece', 'Baghdad', 'Iraq'],\n",
       " ['Athens', 'Greece', 'Bangkok', 'Thailand'],\n",
       " ['Athens', 'Greece', 'Beijing', 'China'],\n",
       " ['Athens', 'Greece', 'Berlin', 'Germany'],\n",
       " ['Athens', 'Greece', 'Bern', 'Switzerland']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "data_tokenized = [sent.split(\" \") for sent in data]\n",
    "data_tokenized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4b7c0f-fc9e-4ade-a691-86065459765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numericalize\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs  = list(set(flatten(data_tokenized))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9a20a36-d2e7-498e-b8e7-41820b094ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "905"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a7d41a7-08f1-4828-af9b-da240692da56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Banjul',\n",
       " 'Minsk',\n",
       " 'Europe',\n",
       " 'taller',\n",
       " 'going',\n",
       " 'Lima',\n",
       " 'generating',\n",
       " 'Chilean',\n",
       " 'Alaska',\n",
       " 'run']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39f6af19-b9ad-4aec-b4cd-3bd997b83d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {v: idx for idx, v in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c0314e-acc3-4b1d-9da3-37a791a9c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = len(vocabs) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d073c43d-97b5-428f-be25-dc7972b7e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139c56c-ea3f-4802-a01f-c9da32fa26dc",
   "metadata": {},
   "source": [
    "# Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "894693fe-44a5-40ba-a15c-dc1c8ef7378e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[847, 753],\n",
       " [847, 819],\n",
       " [819, 847],\n",
       " [819, 746],\n",
       " [847, 753],\n",
       " [847, 193],\n",
       " [193, 847],\n",
       " [193, 506],\n",
       " [847, 753],\n",
       " [847, 391]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgrams = []\n",
    "\n",
    "#for each corpus\n",
    "for sent in data_tokenized:\n",
    "    #for each sent\n",
    "    for i in range(1, len(sent) - 1): #start from 1 to second last\n",
    "        center_word = word2index[sent[i]]\n",
    "        outside_words = [word2index[sent[i-1]], word2index[sent[i+1]]]  #window_size = 1\n",
    "        for o in outside_words:\n",
    "            skipgrams.append([center_word, o])\n",
    "\n",
    "skipgrams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a57b0a76-e835-49f6-82d6-f7c0a57d9f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, skip_grams):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842fa28-114c-45c1-a7c1-e6d4d468ffba",
   "metadata": {},
   "source": [
    "### Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7034857-88e4-432d-b855-a246e4001139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[151]\n",
      " [292]]\n",
      "Target:  [[125]\n",
      " [268]]\n"
     ]
    }
   ],
   "source": [
    "#testing the method\n",
    "batch_size = 2 # mini-batch size\n",
    "input, label = random_batch(batch_size, skipgrams)\n",
    "\n",
    "print(\"Input: \", input)\n",
    "print(\"Target: \", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2b899-5026-4c5b-b93b-126480b77f6c",
   "metadata": {},
   "source": [
    "## Model Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9fa3faf-78a8-4ef6-9772-d8835d7a812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model will accept three vectors - u_o, v_c, u_w\n",
    "#u_o - vector for outside words\n",
    "#v_c - vector for center word\n",
    "#u_w - vectors of all vocabs\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center_word  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_word, outside_word, all_vocabs):\n",
    "        #center_word, outside_word: (batch_size, 1)\n",
    "        #all_vocabs: (batch_size, voc_size)\n",
    "        \n",
    "        #convert them into embedding\n",
    "        center_word_embed  = self.embedding_center_word(center_word)     #(batch_size, 1, emb_size)\n",
    "        outside_word_embed = self.embedding_outside_word(outside_word)   #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embed   = self.embedding_outside_word(all_vocabs)     #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        #bmm is basically @ or .dot , but across batches (i.e., ignore the batch dimension)\n",
    "        top_term = outside_word_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) ===> (batch_size, 1)\n",
    "        \n",
    "        top_term_exp = torch.exp(top_term)  #exp(uo vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        lower_term = all_vocabs_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "         #(batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size)\n",
    "         \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1) #sum exp(uw vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        loss_fn = -torch.mean(torch.log(top_term_exp / lower_term_sum))\n",
    "        #(batch_size, 1) / (batch_size, 1) ==mean==> scalar\n",
    "        \n",
    "        return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d9760ca-3068-426c-8433-f180277828fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 906])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing all_vocabs\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "voc_size = len(vocabs)\n",
    "voc_size\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    #map(function, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e83240-eea8-461e-86d6-8f722378a972",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd06ab79-8a6f-437c-ac57-c9f4e1996291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f6585db-3d68-445c-8410-92757e07ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size  = 5\n",
    "model_Skipgram    = Skipgram(voc_size, emb_size)\n",
    "optimizer = optim.Adam(model_Skipgram.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7715bee-df04-46b5-9981-44a0e384f710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss: 8.073089 | Time: 0.016032695770263672\n",
      "Epoch 2000 | Loss: 7.385178 | Time: 0.020861387252807617\n",
      "Epoch 3000 | Loss: 9.267972 | Time: 0.016358613967895508\n",
      "Epoch 4000 | Loss: 6.753740 | Time: 0.016054868698120117\n",
      "Epoch 5000 | Loss: 6.438174 | Time: 0.01591181755065918\n",
      "Epoch 6000 | Loss: 6.462528 | Time: 0.015711545944213867\n",
      "Epoch 7000 | Loss: 5.139174 | Time: 0.015846967697143555\n",
      "Epoch 8000 | Loss: 6.303550 | Time: 0.015986919403076172\n",
      "Epoch 9000 | Loss: 8.994142 | Time: 0.015226602554321289\n",
      "Epoch 10000 | Loss: 6.228723 | Time: 0.016278505325317383\n",
      "Total trainig time: 161.51635265350342\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "\n",
    "train_start_time = time.time()\n",
    "\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch(batch_size, skipgrams)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "    \n",
    "    # print(input_batch.shape, label_batch.shape, all_vocabs.shape)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = model_Skipgram(input_batch, label_batch, all_vocabs)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: {total_time}\")\n",
    "        \n",
    "total_training_time = time.time() - train_start_time\n",
    "print(\"Total trainig time:\", total_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de520c-fbf0-4981-aa06-aea610a8b7d2",
   "metadata": {},
   "source": [
    "# Skipgram Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54746be0-a1b9-47e0-b021-2c2af36db646",
   "metadata": {},
   "source": [
    "## Unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da4eea9e-b204-4d72-88b2-bf8c06cd28f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78176"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "z = 0.001\n",
    "word_count = Counter(flatten(data_tokenized))\n",
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "num_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7c6ef7a-1322-4e58-b2ba-7d73917f1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for v in vocabs:\n",
    "    uw = word_count[v]/num_total_words\n",
    "    uw_alpha = uw ** 0.75\n",
    "    uw_alpha_dividebyz = int(uw_alpha / z)\n",
    "    unigram_table.extend([v] * uw_alpha_dividebyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba8314c5-f88f-4fbb-bf46-f9d87db20610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    #map(function, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04924ae1-15aa-4d6d-91e6-8da1bc4e6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#you don't want to pick samples = targets, basically negative samples\n",
    "#k = number of negative samples - how many? they found 10 is the best\n",
    "#will be run during training\n",
    "#after random_batch, \n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    #targets is already in id.....\n",
    "    #but the unigram_table is in word....\n",
    "    #1. get the batch size of this targets\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    #2. for each batch\n",
    "    for i in range(batch_size):\n",
    "        #randomly pick k negative words from unigram_table\n",
    "        target_index = targets[i].item()  #looping each of the batch....\n",
    "        nsample = []\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            #if this word == target, skip this word\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        #append this word to some list\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))  #tensor[], tensor[]\n",
    "    return torch.cat(neg_samples)  #tensor[[], []]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447b548-bcc6-4f1f-b7ef-85ffc59345f3",
   "metadata": {},
   "source": [
    "## Model Skipgram Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2128765f-751c-47e0-9087-0ad5e2abc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center_word  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "        \n",
    "    def forward(self, center_words, outside_words, negative_words):\n",
    "        #center_words, outside_words: (batch_size, 1)\n",
    "        #negative_words:  (batch_size, k)\n",
    "        \n",
    "        center_embed  = self.embedding_center_word(center_words)    #(batch_size, 1, emb_size)\n",
    "        outside_embed = self.embedding_outside_word(outside_words)  #(batch_size, 1, emb_size)\n",
    "        neg_embed     = self.embedding_outside_word(negative_words) #(batch_size, k, emb_size)\n",
    "        \n",
    "        uovc          =  outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2)  #(batch_size, 1)\n",
    "        ukvc          = -neg_embed.bmm(center_embed.transpose(1, 2)).squeeze(2)  #(batch_size, k)\n",
    "        ukvc_sum      =  torch.sum(ukvc, 1).view(-1, 1) #(batch_size, 1)\n",
    "        \n",
    "        loss = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)  #(batch_size, 1) + (batch_size, 1)\n",
    "                \n",
    "        return -torch.mean(loss)  #scalar, loss should be scalar, to call backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ceb75a-d054-4fce-b674-a2ab63f3a431",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68222f1e-025b-41a9-a0c5-4ca42f4e0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = random_batch(batch_size,skipgrams)\n",
    "input_tensor = torch.LongTensor(input)  \n",
    "label_tensor = torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdd0deb4-3344-4e32-a1f9-defedd853d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2\n",
    "voc_size = len(vocabs)\n",
    "model = SkipgramNeg(voc_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cb007b9-5d9c-4362-973d-d4acb81df551",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tensor = negative_sampling(label_tensor, unigram_table, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6cd22b6-e447-4115-b5d2-4163ba5057c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1]), torch.Size([5, 1]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape, label_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1985894b-fa94-4ca3-aec0-51c9045b1106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3530, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model(input_tensor, label_tensor, neg_tensor)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ead0a-042b-4c42-8bcf-b66e4c89bd5e",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9f5e0b6-3f52-4bd3-aa8d-02abe4ef9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 5 #so we can later plot\n",
    "model_SkipgramNeg  = SkipgramNeg(voc_size, emb_size)\n",
    "num_neg        = 10 # num of negative sampling\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0828fc1d-3fc0-4607-8647-9dcc451a69d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkipgramNeg(\n",
       "  (embedding_center_word): Embedding(906, 5)\n",
       "  (embedding_outside_word): Embedding(906, 5)\n",
       "  (logsigmoid): LogSigmoid()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SkipgramNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41484006-3f5c-4f51-bc5a-de1b7cac36f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss: 5.413135 | Time: 0.014811992645263672\n",
      "Epoch 2000 | Loss: 3.879498 | Time: 0.014837980270385742\n",
      "Epoch 3000 | Loss: 7.471133 | Time: 0.014683961868286133\n",
      "Epoch 4000 | Loss: 4.606218 | Time: 0.014765024185180664\n",
      "Epoch 5000 | Loss: 4.508828 | Time: 0.02836155891418457\n",
      "Total trainig time: 300.11318278312683\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "init_time = time.time()\n",
    "\n",
    "# Training\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    input_batch, target_batch = random_batch(batch_size, skipgrams)\n",
    "    \n",
    "    #input_batch: [batch_size, 1]\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    \n",
    "    #target_batch: [batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "    \n",
    "    #negs_batch:   [batch_size, num_neg]\n",
    "    negs_batch = negative_sampling(target_batch, unigram_table, num_neg)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    loss = model_SkipgramNeg(input_batch, target_batch, negs_batch)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: {total_time}\")\n",
    "        \n",
    "total_training_time = time.time() - train_start_time\n",
    "print(\"Total trainig time:\", total_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90846ce5-1665-459a-917c-095e95a9198c",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41f60d57-8759-48ef-a6f9-10da30abd5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[847, [753, 819]],\n",
       " [819, [847, 746]],\n",
       " [847, [753, 193]],\n",
       " [193, [847, 506]],\n",
       " [847, [753, 391]],\n",
       " [391, [847, 47]],\n",
       " [847, [753, 255]],\n",
       " [255, [847, 303]],\n",
       " [847, [753, 403]],\n",
       " [403, [847, 716]]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbows = []\n",
    "\n",
    "#for each corpus\n",
    "for sent in data_tokenized:\n",
    "    #for each sent\n",
    "    for i in range(1, len(sent) - 1): #start from 1 to second last\n",
    "        center_word = word2index[sent[i]]\n",
    "        \n",
    "        outside_words = []\n",
    "        low  = i - 1\n",
    "        high = i + 1\n",
    "        for j in range(low, high + 1):\n",
    "            if j == i:\n",
    "                continue\n",
    "            outside_words.append(word2index[sent[j]])\n",
    "        cbows.append([center_word, outside_words])\n",
    "        \n",
    "cbows[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "279b2fb0-8fde-4ab5-aae0-8ad1544cdd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch_cbow(batch_size, cbows):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(cbows)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([cbows[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([cbows[i][1]])  # context word, e.g., 3\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0362532-086d-4910-9059-63d63feb59a5",
   "metadata": {},
   "source": [
    "## Model CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2dca77-664a-47c3-a0ef-13782ce23d40",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- http://cs224d.stanford.edu/lecture_notes/notes1.pdf\n",
    "- Took help from Abhinav's Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd5dc43a-f0b1-449b-8d36-834387b86fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.input_word  = nn.Embedding(voc_size, emb_size) #v\n",
    "        self.output_word = nn.Embedding(voc_size, emb_size) #u\n",
    "    \n",
    "    def forward(self, center_word, outside_word, all_vocabs):\n",
    "        #center_word: (batch_size, 1)\n",
    "        #context_words: (batch_size, window_size * 2)\n",
    "        #all_vocabs: (batch_size, voc_size)\n",
    "        batch_size = center_word.shape[0]\n",
    "        \n",
    "        #convert them into embedding\n",
    "        center_word_embed  = self.output_word(center_word)   #(batch_size, 1, emb_size)\n",
    "        outside_word_embed = self.input_word(outside_word)   #(batch_size, window_size * 2, emb_size)\n",
    "        all_vocabs_embed   = self.output_word(all_vocabs)    #(batch_size, voc_size, emb_size)        \n",
    "        \n",
    "        # mean of input word embeddings\n",
    "        v_mean = torch.sum(outside_word_embed, 1) / len(outside_word) #(batch_size, emb_size)\n",
    "        \n",
    "        ucv = center_word_embed.bmm(v_mean.reshape(batch_size, 1, self.emb_size).transpose(1, 2)).squeeze()\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) ==> (batch_size, 1)\n",
    "        \n",
    "        ujv = all_vocabs_embed.bmm(v_mean.reshape(batch_size, 1, self.emb_size).transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size)\n",
    "        \n",
    "        ujv_log_exp = torch.log(torch.exp(ujv))\n",
    "        # (batch_size, voc_size) -> (batch_size,)\n",
    "        \n",
    "        loss_fn = - ucv + torch.sum(ujv_log_exp, 1)\n",
    "        # - (batch_size, 1) + (batch_size, 1) = (batch_size, 1)\n",
    "        \n",
    "        return torch.mean(loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960a0675-1b91-4497-a5b9-d11fc3807f89",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59584f61-4f54-41c4-a95a-5f17d4571608",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2\n",
    "model_cbow = CBOW(voc_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2045a296-1186-48f9-a724-15eaff6a33a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (input_word): Embedding(906, 2)\n",
       "  (output_word): Embedding(906, 2)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21715d7b-007a-470f-bb38-f0e3397bc19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss: 3.804660 | Time: 0.012360334396362305\n",
      "Epoch 2000 | Loss: -5.989528 | Time: 0.009416818618774414\n",
      "Epoch 3000 | Loss: -2.011968 | Time: 0.00953364372253418\n",
      "Epoch 4000 | Loss: -2.777016 | Time: 0.009201288223266602\n",
      "Epoch 5000 | Loss: -0.894053 | Time: 0.009268045425415039\n",
      "Total trainig time: 48.29684400558472\n"
     ]
    }
   ],
   "source": [
    "init_time = time.time()\n",
    "\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch_cbow(batch_size, cbows)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = model_cbow(input_batch, label_batch, all_vocabs)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: {total_time}\")\n",
    "        \n",
    "total_training_time = time.time() - init_time\n",
    "print(\"Total trainig time:\", total_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a79e9d-8d49-4082-8a23-929e1d630071",
   "metadata": {},
   "source": [
    "# GLOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8f1f4-237a-47a4-9c7a-433279cc8886",
   "metadata": {},
   "source": [
    "## Co-occurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0d06712-6495-47aa-99ba-78b9b71491e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#count the frequency of each word....\n",
    "X_i = Counter(flatten(data_tokenized)) #merge all list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c450ca7-c3eb-4cd9-86cc-f98fd8674aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Greece', 'Baghdad'),\n",
       " ('Greece', 'Athens'),\n",
       " ('Baghdad', 'Iraq'),\n",
       " ('Baghdad', 'Greece'),\n",
       " ('Greece', 'Bangkok'),\n",
       " ('Greece', 'Athens'),\n",
       " ('Bangkok', 'Thailand'),\n",
       " ('Bangkok', 'Greece'),\n",
       " ('Greece', 'Beijing'),\n",
       " ('Greece', 'Athens')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gloves = []\n",
    "\n",
    "#loop through each corpus\n",
    "for sent in data_tokenized: \n",
    "    #loop through each word from 1 to n-1 (because 0 and n has no context window)\n",
    "    for i in range(1, len(sent)-1):\n",
    "        target  = sent[i]\n",
    "        context = [sent[i+1], sent[i-1]]\n",
    "        #append(i, i+1) and append(i, i-1)\n",
    "        for c in context:\n",
    "            gloves.append((target, c))\n",
    "\n",
    "gloves[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f529cc3-d5cd-4dba-ab3f-98b225e24f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we have these occurrences, we can count, to make our co-occurrence matrix!!!\n",
    "X_ik_skipgram = Counter(gloves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03e57c8e-e286-4671-ae0b-3d30bf7b9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_ik_skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810c601d-86c7-4246-8271-7035183d698e",
   "metadata": {},
   "source": [
    "## Weighting function f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e5df993-e548-49f1-9290-e61dd381b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j, X_ik):   #why we need w_i and w_j, because we can try its co-occurrences, if it's too big, we scale it down\n",
    "    \n",
    "    #check whether the co-occurrences between these two word exists???\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1  #why one, so that the probability thingy won't break...(label smoothing)\n",
    "        \n",
    "    #maximum co-occurrences; we follow the paper\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if the co-occurrences does not exceed x_max, scale it down based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max) ** alpha\n",
    "    else:\n",
    "        result = 1 #this is the maximum probability you can have\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "779f44db-34b5-4a58-ad2f-3d0260ca747c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6902356338456498\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "w_i  = 'Baghdad'\n",
    "w_j  = 'Iraq'\n",
    "w_j2 = 'demo'\n",
    "\n",
    "print(weighting(w_i, w_j, X_ik_skipgram))   #scales from 1 to 0.0316\n",
    "print(weighting(w_i, w_j2, X_ik_skipgram))  #the paper says that f(0) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a0eff3a-fb95-4dbf-bc1d-d5913be86eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now apply this weighting to all possible pairs\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #for keeping the co-occurrences\n",
    "weighting_dic = {} #for keeping all the probability after passing through the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2):  #we need to also think its reverse\n",
    "    #if this bigram exists in X_ik_skipgrams\n",
    "    #we gonna add this to our co-occurence matrix\n",
    "    if X_ik_skipgram.get(bigram) is not None:\n",
    "        cooc = X_ik_skipgram[bigram]  #get the co-occurrence\n",
    "        X_ik[bigram] = cooc + 1 #this is again basically label smoothing....(stability issues (especially when divide something))\n",
    "        X_ik[(bigram[1], bigram[0])] = cooc + 1  #trick to get all pairs\n",
    "    else: #otherwise, do nothing\n",
    "        pass\n",
    "    \n",
    "    #apply the weighting function using this co-occurrence matrix thingy    \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c70717db-d9fa-4b59-a05f-37f47161cc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37266"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_ik_skipgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332fd7f0-e50b-4332-941c-780b904b7de2",
   "metadata": {},
   "source": [
    "## Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd7e9fe1-643e-460a-a8fa-b839475c9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch_glove(batch_size, word_sequence, gloves, X_ik, weighting_dic):\n",
    "    \n",
    "    #loop through this skipgram, and change it id  because when sending model, it must number\n",
    "    gloves_id = [(word2index[g[0]], word2index[g[1]]) for g in gloves]\n",
    "    \n",
    "    #randomly pick \"batch_size\" indexes\n",
    "    number_of_choices = len(gloves_id)\n",
    "    random_index = np.random.choice(number_of_choices, batch_size, replace=False) #no repeating indexes among these random indexes\n",
    "    \n",
    "    random_inputs = [] #xi, wi (in batches)\n",
    "    random_labels = [] #xj, wj (in batches)\n",
    "    random_coocs  = [] #Xij (in batches)\n",
    "    random_weighting = [] #f(Xij) (in batches)\n",
    "    #for each of the sample in these indexes\n",
    "    for i in random_index:\n",
    "        random_inputs.append([gloves_id[i][0]]) #same reason why i put bracket here....\n",
    "        random_labels.append([gloves_id[i][1]])\n",
    "        \n",
    "        #get cooc\n",
    "        #first check whether it exists...\n",
    "        pair = gloves[i]  #e.g., ('banana', 'fruit)\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1 #label smoothing\n",
    "            \n",
    "        random_coocs.append([math.log(cooc)])  #1. why log, #2, why bracket -> size ==> (, 1)  #my neural network expects (, 1)\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]  #why not use try....maybe it does not exist....\n",
    "        random_weighting.append(weighting)\n",
    "\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0be92bec-097a-4fa3-bdbb-ffe4d582d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "input, target, cooc, weightin = random_batch_glove(batch_size, data_tokenized, gloves, X_ik, weighting_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73673942-c1b3-469c-9ed8-d10cc45e497f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 13],\n",
       "        [223]]),\n",
       " array([[854],\n",
       "        [239]]),\n",
       " array([[0.69314718],\n",
       "        [0.69314718]]),\n",
       " array([0.05318296, 0.05318296]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target, cooc, weightin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e27b96-feef-4bf5-a59b-8b9b25dff106",
   "metadata": {},
   "source": [
    "## Model GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b3780e6-86e7-460e-9a6b-7d13d0d1ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size) # out embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        \n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs already got log\n",
    "        loss = weighting*torch.pow(inner_product +center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef90fd8-a15b-454f-b7c2-f49503872c15",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "590d66a5-c963-4a07-95d0-7451e06ff618",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size   = len(vocabs)\n",
    "batch_size = 2 #why?  no reason; \n",
    "emb_size   = 5 #why?  no reason; usually 50, 100, 300, but 2 so we can plot (50 can also plot, but need PCA)\n",
    "model_glove      = GloVe(voc_size, emb_size)\n",
    "\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c4eb939-9fbe-4672-b5ae-384a77fbf819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GloVe(\n",
       "  (embedding_v): Embedding(906, 5)\n",
       "  (embedding_u): Embedding(906, 5)\n",
       "  (v_bias): Embedding(906, 1)\n",
       "  (u_bias): Embedding(906, 1)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bf60fa9-0746-47ac-b31a-8953329d537b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss: 3.794889 | Time: 0.03260374069213867\n",
      "Epoch 2000 | Loss: 12.852983 | Time: 0.03130650520324707\n",
      "Epoch 3000 | Loss: 36.234222 | Time: 0.03966832160949707\n",
      "Epoch 4000 | Loss: 0.972803 | Time: 0.03301095962524414\n",
      "Epoch 5000 | Loss: 2.203818 | Time: 0.03419756889343262\n",
      "Total trainig time: 161.96291208267212\n"
     ]
    }
   ],
   "source": [
    "init_time = time.time()\n",
    "\n",
    "num_epochs = 5000\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    #get random batch\n",
    "    input, target, cooc, weightin = random_batch_glove(batch_size, data_tokenized, gloves, X_ik, weighting_dic)\n",
    "    input_batch    = torch.LongTensor(input)\n",
    "    target_batch   = torch.LongTensor(target)\n",
    "    cooc_batch     = torch.FloatTensor(cooc)\n",
    "    weightin_batch = torch.FloatTensor(weightin)\n",
    "    \n",
    "    \n",
    "    # print(input_batch.shape, label_batch.shape, cooc_batch.shape, weightin_batch)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = model_glove(input_batch, target_batch, cooc_batch, weightin_batch)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    total_time = time.time() - start_time\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: {total_time}\")\n",
    "\n",
    "        \n",
    "total_training_time = time.time() - init_time\n",
    "print(\"Total trainig time:\", total_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cddc57-a9df-4b39-b793-219f55a507e1",
   "metadata": {},
   "source": [
    "# Part 1: Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3403db53-35b4-4ef8-b956-8b6661ef399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_Skipgram\n",
    "# model_SkipgramNeg\n",
    "# model_cbow\n",
    "# model_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edeb6af-8811-4d97-aff9-add8d6575a03",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c8bbe26c-6ff5-4031-9a5d-d3454135259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('capital-world.txt') as file:\n",
    "    test_data = [line.strip() for line in file.readlines() if line[0] != ':']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ff8a7891-281d-4211-a49f-c2458ae1d99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4524"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ef31cd20-0213-4c37-b944-4bca754b154a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abuja nigeria accra ghana',\n",
       " 'abuja nigeria algiers algeria',\n",
       " 'abuja nigeria amman jordan',\n",
       " 'abuja nigeria ankara turkey',\n",
       " 'abuja nigeria antananarivo madagascar']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8193c633-edfa-4458-a3f7-919fabbe8d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['abuja', 'nigeria', 'accra', 'ghana'],\n",
       " ['abuja', 'nigeria', 'algiers', 'algeria'],\n",
       " ['abuja', 'nigeria', 'amman', 'jordan'],\n",
       " ['abuja', 'nigeria', 'ankara', 'turkey'],\n",
       " ['abuja', 'nigeria', 'antananarivo', 'madagascar']]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "test_data_tokenized = [sent.split(\" \") for sent in test_data]\n",
    "test_data_tokenized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "863b6d54-dd08-4e06-ac37-13e8dc6ab09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "test_vocabs  = list(set(flatten(test_data_tokenized))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "66780ef8-8b48-40a7-a351-9229be433670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\n",
      "['venezuela', 'sudan', 'lilongwe', 'qatar', 'suva']\n"
     ]
    }
   ],
   "source": [
    "print(len(test_vocabs))\n",
    "print(test_vocabs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e39df7d9-0fc5-4132-85b3-6bd5f9bac311",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {v: idx for idx, v in enumerate(test_vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8e6ffed3-e6bd-466e-b706-051bcc6930e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = len(test_vocabs) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5427b74a-7378-4278-9293-a6b48a644e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e18458-7e2f-40bc-9822-4eea3ab9ed94",
   "metadata": {},
   "source": [
    "## Semantic accuracy (Skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "90754ea3-df5b-4de7-b64a-72e3590f7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's write a function to get embedding given a word\n",
    "def get_embed(model, word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    v_embed = model.embedding_center_word(id_tensor)\n",
    "    u_embed = model.embedding_outside_word(id_tensor) \n",
    "    # print(v_embed)\n",
    "    # print(u_embed)\n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    #x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return word_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "194540f8-e76e-46b5-8b94-9e2de89847b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for word in test_vocabs: #loop each unique vocab\n",
    "    embeddings.append(get_embed(model_Skipgram, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "2ba92172-f324-44c4-a423-7fb383094d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "a1c9a4dc-09cd-4f53-be00-64539a4b460e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6268, 0.2669, 0.0081, 0.2252, 0.6153]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embed = get_embed(model_Skipgram, test_vocabs[0])\n",
    "test_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "d143124b-8c14-43c9-afca-8f29a85c84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings[:5]\n",
    "# Convert the list of embeddings to a tensor\n",
    "embeddings = torch.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "b279697a-e9bf-40dc-b891-25fc7e2c63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(embeddings) - 1):\n",
    "#     for j in range(i+1, len(embeddings) - 1):\n",
    "#         count = 0\n",
    "#         similarity = torch.nn.CosineSimilarity(embeddings[i].detach().numpy().squeeze(), embeddings[j].detach().numpy().squeeze())\n",
    "#         print(similarity)\n",
    "#         if i == 0:\n",
    "#             break\n",
    "#         count += 1\n",
    "#     #similarity = similarity/count\n",
    "\n",
    "# similarity = similarity / len(embeddings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "288e7756-ebcf-4998-92c1-a89a0ceebef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #numpy version\n",
    "# from numpy import dot\n",
    "# from numpy.linalg import norm\n",
    "\n",
    "# def cos_sim(a, b):\n",
    "#     # a = a.detach().numpy()\n",
    "#     # b = b.detach().numpy()\n",
    "#     cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "#     return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "4987a3cc-c5d3-4101-b363-a5536c542dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(embeddings) - 1):\n",
    "#     for j in range(i+1, len(embeddings) - 1):\n",
    "#         similarity = cos_sim(embeddings[i], embeddings[j])\n",
    "#         #print(similarity)\n",
    "#         if i == 0:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "126000ae-385f-4f5f-bb90-82c03475b572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the semantic similarity using cosine similarity\n",
    "similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "#print(similarity)\n",
    "\n",
    "# Compute the average similarity between all pairs of words\n",
    "accuracy = similarity(embeddings, embeddings).mean()\n",
    "\n",
    "print('Semantic accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7b721-3041-43f0-8d2a-90e0e0a768fe",
   "metadata": {},
   "source": [
    "## Semantic accuracy (Skipgram Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8b7d13e2-e14b-4430-9a63-69ae010e8b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for word in test_vocabs: #loop each unique vocab\n",
    "    embeddings.append(get_embed(model_SkipgramNeg, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "8af52f78-1ded-4a8b-b860-174cedfaafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of embeddings to a tensor\n",
    "embeddings = torch.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "6284989b-2ca3-4c6c-9e03-e22b9254cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the semantic similarity using cosine similarity\n",
    "similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "# Compute the average similarity between all pairs of words\n",
    "accuracy = similarity(embeddings, embeddings).mean()\n",
    "\n",
    "print('Semantic accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68462df-a2d2-4e14-9602-7bdfd7d21bf4",
   "metadata": {},
   "source": [
    "## Semantic accuracy (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e9248909-ee85-4dee-8803-b5a6a45e252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's write a function to get embedding given a word\n",
    "def get_embed_cbow(model, word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    v_embed = model.input_word(id_tensor)\n",
    "    u_embed = model.output_word(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    #x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return word_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "875cdc6e-f34f-4430-9963-e3f1f9518598",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for word in test_vocabs: #loop each unique vocab\n",
    "    embeddings.append(get_embed_cbow(model_cbow, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9046659a-21ae-45db-9972-a98b3d912082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of embeddings to a tensor\n",
    "embeddings = torch.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "aabfe2ee-8bc1-470b-945b-3178f1df5371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the semantic similarity using cosine similarity\n",
    "similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "# Compute the average similarity between all pairs of words\n",
    "accuracy = similarity(embeddings, embeddings).mean()\n",
    "\n",
    "print('Semantic accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135261c-9e6f-466c-850e-0c718303cb1e",
   "metadata": {},
   "source": [
    "## Semantic accuracy (GLOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "01cf1d5d-367a-4fae-9761-9fba15e8fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's write a function to get embedding given a word\n",
    "def get_embed_glove(model, word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    v_embed = model.embedding_v(id_tensor)\n",
    "    u_embed = model.embedding_u(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    #x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "    return word_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "08899c8e-d18c-48ce-af34-7547ca27ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for word in test_vocabs: #loop each unique vocab\n",
    "    #embeddings.append(get_embed_glove(model_glove, word).squeeze())\n",
    "    embed = get_embed_glove(model_glove, word).squeeze()\n",
    "    embeddings.append(embed.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "00ab802f-0edd-46ed-bec8-57bf985c0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of embeddings to a tensor\n",
    "#embeddings = torch.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0ad49ecd-bba9-4f48-9d7f-840e6c22b21d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (233,5) and (233,5) not aligned: 5 (dim 1) != 233 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_148/4218219444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#accuracy = similarity(embeddings, embeddings).mean()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Semantic accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_148/3002580537.py\u001b[0m in \u001b[0;36mcos_sim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcos_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (233,5) and (233,5) not aligned: 5 (dim 1) != 233 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Compute the semantic similarity using cosine similarity\n",
    "#similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "# Compute the average similarity between all pairs of words\n",
    "#accuracy = similarity(embeddings, embeddings).mean()\n",
    "\n",
    "accuracy = cos_sim(embeddings, embeddings).mean()\n",
    "print('Semantic accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa2e73-6c15-4578-b56e-435fbbaef1e5",
   "metadata": {},
   "source": [
    "# Part 2: Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7b333a5e-53c6-45b4-9136-f94705d48e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a770487-4402-4b71-a16a-5e1108083d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wordsim_similarity_goldstandard.txt') as f:\n",
    "    data_similarity = [line.strip().lower().split(\"\\t\") for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "75b2ed63-0e5a-427c-8a79-e23da56568c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tiger', 'cat', '7.35'],\n",
       " ['tiger', 'tiger', '10.00'],\n",
       " ['plane', 'car', '5.77'],\n",
       " ['train', 'car', '6.31'],\n",
       " ['television', 'radio', '6.77']]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_similarity[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "41f93418-5532-4a63-9a43-170d735868fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "test_similarity_vocabs  = list((flatten(data_similarity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "18a91088-7e91-4d60-9f06-f81fd6079285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "609"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_similarity_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dc8aaf0b-5d78-4497-8264-859073c49916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tiger', 'cat', '7.35', 'tiger', 'tiger', '10.00']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_similarity_vocabs[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f90a137d-c96e-44cc-b180-6bdf5f17cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {v: idx for idx, v in enumerate(test_similarity_vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f1a88cc3-8bad-4a6c-bdd1-ff8bd15d29a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801435f-7fd1-4f9b-9d70-3116853c6c71",
   "metadata": {},
   "source": [
    "## CBOW Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d6336d-5ee1-41e7-9468-ed8687797e9a",
   "metadata": {},
   "source": [
    "spearmanr: https://www.statology.org/spearman-correlation-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "99b52269-f0ae-4a6d-acc4-8a3b5a4233aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: SpearmanrResult(correlation=-0.1754386335267139, pvalue=0.4468597854046624)\n"
     ]
    }
   ],
   "source": [
    "# Read the similarity dataset\n",
    "similarities = []\n",
    "with open('wordsim_similarity_goldstandard.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        word1, word2, similarity = line.strip().split(\"\\t\")\n",
    "        word1_embedding = get_embed_cbow(model_cbow, word1).squeeze()\n",
    "        word2_embedding = get_embed_cbow(model_cbow, word2).squeeze()\n",
    "        cosine_similarity = cos_sim(word1_embedding.detach().numpy(), word2_embedding.detach().numpy())\n",
    "        similarities.append((cosine_similarity, float(similarity)))\n",
    "\n",
    "# Calculate the spearman correlation\n",
    "spearman_correlation = spearmanr([x[0] for x in similarities], [x[1] for x in similarities])\n",
    "print('Spearman correlation:', spearman_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b8f7a-2171-477b-a271-0b7e9befbad8",
   "metadata": {},
   "source": [
    "## GLOVE similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b70f35c6-0507-473a-9a69-d80a870895fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: SpearmanrResult(correlation=0.07927227144540408, pvalue=0.7326782054021117)\n"
     ]
    }
   ],
   "source": [
    "# Read the similarity dataset\n",
    "similarities = []\n",
    "with open('wordsim_similarity_goldstandard.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        word1, word2, similarity = line.strip().split(\"\\t\")\n",
    "        word1_embedding = get_embed_glove(model_glove, word1).squeeze()\n",
    "        word2_embedding = get_embed_glove(model_glove, word2).squeeze()\n",
    "        cosine_similarity = cos_sim(word1_embedding.detach().numpy(), word2_embedding.detach().numpy())\n",
    "        similarities.append((cosine_similarity, float(similarity)))\n",
    "\n",
    "# Calculate the spearman correlation\n",
    "spearman_correlation = spearmanr([x[0] for x in similarities], [x[1] for x in similarities])\n",
    "print('Spearman correlation:', spearman_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49357b9a-a46a-4a92-87df-edd3a494621d",
   "metadata": {},
   "source": [
    "## Skipgram Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "045b8ad3-a853-405a-8ec5-af3a4b8e9229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: SpearmanrResult(correlation=0.31448999491455387, pvalue=0.1649985541392634)\n"
     ]
    }
   ],
   "source": [
    "# Read the similarity dataset\n",
    "similarities = []\n",
    "with open('wordsim_similarity_goldstandard.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        word1, word2, similarity = line.strip().split(\"\\t\")\n",
    "        word1_embedding = get_embed(model_Skipgram, word1).squeeze()\n",
    "        word2_embedding = get_embed(model_Skipgram, word2).squeeze()\n",
    "        cosine_similarity = cos_sim(word1_embedding.detach().numpy(), word2_embedding.detach().numpy())\n",
    "        similarities.append((cosine_similarity, float(similarity)))\n",
    "\n",
    "# Calculate the spearman correlation\n",
    "spearman_correlation = spearmanr([x[0] for x in similarities], [x[1] for x in similarities])\n",
    "print('Spearman correlation:', spearman_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265dbad9-638a-4142-b2bc-c3c11f9fcb3f",
   "metadata": {},
   "source": [
    "### Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40659b71-022d-4286-9e2b-700e04de9fa0",
   "metadata": {},
   "source": [
    "In terms of correlation Skipgram and CBOW are showing some good correlation with the values 0.31 and -0.18 respectively. The reason might be that I trained Skipgram for 10000 epochs, but later on due to some issues in the puffer (it was taking too long to train) I trained other models for 5000 epochs. Although P value for all methods are showing significance. This might happen due to the less amount of data that we trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77b800-6253-444b-81de-f49b3f6483e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
